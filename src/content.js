import React from 'react';
import {Flex, Col, Row, Text, Image, Link} from './components/common/base';
import { unlimitColor } from './styles';
import as2_1 from './imgs/0315-1.png';
import as2_2 from './imgs/0315-2.png';
import as2_3 from './imgs/0315-3.png';
import as3_1 from './imgs/0331-1.png';
import as3_2 from './imgs/0331-2.png';
import as3_3 from './imgs/0331-3.png';


export const contents = [
    {
        title: "",
        description: 
            <Col align="center" justify="center" style={{alignSelf: "center", width: '100%', height: '100%'}}>
                <Image style={{width: 100, objectFit: 'contain'}} src="https://upload.wikimedia.org/wikipedia/en/thumb/f/f6/Korea_University_Global_Symbol.svg/1200px-Korea_University_Global_Symbol.svg.png"  />
                <Text size="35" weight="700" margin="margin: 15px;" style={{fontFamily: 'Cormorant Garamond'}}>Ethics of Artificial Intelligence</Text>
                <Text size="22" style={{fontStyle: "italic", fontFamily: 'Cormorant Garamond'}}>Prof. Hyunchul Kim</Text>
            </Col>
        ,
    },
    {
        title: "",
        description: 
            <Col align="center" justify="center" style={{alignSelf: "center", width: '100%', height: '100%'}}>
                <Text size="18" style={{fontStyle: "italic", textAlign: "center"}} margin="margin: 20px;" lh="40">"이 노트를 김현철 교수님과 학우들께 바칩니다.</Text>
                <Text size="18" style={{fontStyle: "italic", textAlign: "center"}} lh="40">한 학기 동안 김현철 교수님의 인공지능과윤리 수업을 정리하고, 또 제가 제출한 과제를 바탕으로 정리한 웹노트입니다. <br></br> 미약하지만, 추후에 공부할 후배들과 인공지능응용전공 학우들에게 많은 도움이 됬으면 좋겠습니다."</Text>
                <Text size="18" style={{opacity: 0.8}} lh="40" margin="margin-top: 20px;">Written by 최수형, 2018390740</Text>
                <Text size="18" style={{opacity: 0.8}} margin="margin-top: 5px;">Github: <Link target="_blank" href="https://www.github.com/jonychoi/ai-ethics">ai-ethics</Link><Link target="_blank" style={{marginLeft: 10}} href="https://www.github.com/jonychoi">@jonychoi</Link></Text>
            </Col>
    },
    {
        title: "Introduction to Ethics of Artificial Intelligence",
        //description: "많은 부분에 ai 침투, ai 개발하면서 vision lab에서 일하고 있음으로써 여러 관점에서 윤리적인 측면을 파악하고자함. 내용은 수업 내용을 기반으로 더 조사하여 보강하여 "
        // 본수업에서는 A.I의 창발과 그 발전사를 통해 A.I의 연구 개발 현황부터 최신 이슈까지 폭넓게 다뤘으며, 이로부터 발생할 수 있는 다양한 ai ehitcal issue들을 논리적으로 분석하고, 여러 학생들의 의견까지 들어보았다. 
        //읽기 좋은 형태로 만든것임. 해당 컨텐츠는 다음과 같이 4챕터로 이루어져있음 1. What is A.I? The Fundamental of A.I and its Ethics. 2. The Logical Analysis of Ethics of A.I 3. Ethical Issues in practical, My philosihpy: 과제"
        description: 
        <Col align="flex-start" justify="flex-start">
            <Text className="text">
                    <p>
                        우리 일상에서 이미 많은 부분에 있어서 A.I를 이용하여 수많은 것들이 이루어지고 있다. 특히 End-User가 직접 이용할 수 있는 Chat Bot, Face Detection 등 많은 application에서 인공지능을 활용한 것들이 창발 하고 있다. 이처럼 인공지능은 서서히 우리 삶을 바꾸고 있지만, 과연 그것이 우리에게 유익할까?
                    </p>
                    <p>
                        본 수업에서는 A.I의 창발과 그 발전사를 통해 A.I의 연구 개발 현황부터 최신이슈까지 폭녋게 다뤘으며, 이로부터 발생할 수 있는 다양한 A.I Ethical Issue들을 윤리적인 관점으로 바라보고, 그것을 논리적으로 분해해 나간다. 특히 인공지능의 4대 윤리적인 측면, 공정성, 신뢰성, 안정성, 투명성의 관점에서 분석하고, 그것이 향후 어떤 방향으로 발전 할 수 있는 지 심도 있게 알아보았다.
                    </p>
                    <p>
                        본 노트는 수업내용과 책, 그리고 나의 과제를 요약하여 세 챕터로 이루어져있다.
                    </p>
                    <ul>
                        <li>
                            1. What is A.I? The Fundamental of A.I and its Ethics [Stuart Russell - Human-Compatible]
                        </li>
                        <li>
                            2. 4 Aspects of A.I Ethics. [한상기 - 신뢰할 수 있는 인공지능]
                        </li>
                        <li>
                            3. My Thoughts [Assignments]
                        </li>
                    </ul>
                    <p><strong>참고 도서는 아래와 같다.</strong></p>
                    <Row align="center" margin="margin-top: 15px;">
                        <a target="_blank" href="https://en.wikipedia.org/wiki/Human_Compatible">
                            <Col margin="margin-right: 15px;" align="center" justify="center">
                                <Image width="150px" src="http://image.kyobobook.co.kr/images/book/xlarge/613/x9780525558613.jpg" />
                                <p style={{textAlign: 'center'}}><strong>Human Compatible</strong><br></br>Stuart Russell</p>
                            </Col>
                        </a>
                        <a target="_blank" href="http://www.yes24.com/Product/Goods/103707967">
                            <Col margin="margin-right: 15px;" align="center" justify="center">
                                <Image width="150px" src="https://image.yes24.com/goods/103707967/XL"/>
                                <p style={{textAlign: 'center'}}><strong>신뢰할 수 있는 인공지능</strong><br></br>한상기</p>
                            </Col>
                        </a>
                    </Row>
                </Text>
        </Col>,
        date: 'March 3'
    },
    {
        title: "About the A.I Ethics",
        description: <Col>
            <Text className="text">
                <p>
                    인공지능은 인간의 지능적인 행위 (의사결정, 판단, 추론, 예측) 등을 컴퓨팅 모델로 만드는 것이다. 이런 점에서 인공지능의 상당 부분은 인간의 지능을 모방하기 위해서 컴퓨터 과학 뿐만 아니라 인지심리, 철학, 논리학, 수학, 물리학 등 많은 부분에서 영감을 얻고 있다.
                </p>
                <p>
                    A.I의 사전적 정의는 다음과 같다.
                </p>
                <p style={{fontStyle: 'italic'}}>
                    Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by animals including humans. AI research has been defined as the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals.
                    The term "artificial intelligence" had previously been used to describe machines that mimic and display "human" cognitive skills that are associated with the human mind, such as "learning" and "problem-solving". This definition has since been rejected by major AI researchers who now describe AI in terms of rationality and acting rationally, which does not limit how intelligence can be articulated.
                </p>
                <p style={{fontStyle: 'italic'}}>- Wikipedia</p>
                <Row>
                    <Image width="400px" of="contain" src="https://www.analytixlabs.co.in/blog/wp-content/uploads/2021/02/blogs-banner-1-1-1.jpg" />
                    <Col margin="margin-left: 25px;">
                        <p>즉 A.I는 4가지 영역, 문제해결, 추론, 학습, 인식의 부분을 컴퓨팅한 것이며, 이는 인간이 갖고 있는 지능적 행위를 모방하여 이루어진다. 특히 뇌과학에서 많은 영감을 얻어 만들어진 Neural Network는 Deep Learning의 창발을 가져와, 특정 task들에서는 인간을 훨씬 능가하는 수준을 보여주고 있다.</p>
                        <p>그러나 갈수록 발전되고, 심화되는 인공지능 개발과 연구의 속도에 비해서, 그것의 발전 속도를 잘 따라오지 못한 정립되지 않은 사회 제도적, 그리고 윤리적 문제가 많은 우려가 되고 있다. 이는 매우 심각한 문제인데, 향후 더욱더 막대한 영향력을 발휘할 인공지능이 인간과 함께 공존할 수 있는 문제로 야기되는 '특이점'에 관한 논의로 귀결되고도 한다. 이에 '인공지능 윤리'는 인공지능을 인간사회에 정립된 윤리적인 관점으로 바라보고 해석하기 위해 연구되고 있으며, 이는 인공지능에 대한 신뢰성, 편향성, 책임성, 악용가능성 등 많은 관점에서 논의되고 있다.</p>
                    </Col>
                </Row>
                <p>
                    Ethics of artificial intelligence의 사전적 정의는 다음과 같다.
                </p>
                <p style={{fontStyle: 'italic'}}>
                    The ethics of artificial intelligence is the branch of the ethics of technology specific to artificially intelligent systems. It is sometimes divided into a concern with the moral behavior of humans as they design, make, use and treat artificially intelligent systems, and a concern with the behavior of machines, in machine ethics. It also includes the issue of a possible singularity due to superintelligent AI.
                </p>
                <p style={{fontStyle: 'italic'}}>- Wikipedia</p>
                <p>
                    즉, 인공지능 윤리는 단순히 인간사회에서 나타날 수 있는 다양한 사회적, 윤리적 문제 뿐만 아니라, 향후 도래할 가능성이 있는 초지능, 그리고 인공지능의 발전으로 인해서 일어날 인간성에 대한 폭 넓은 논의가 될 것이다. 우리는 이제부터 이를 하나씩 분해해 나가며 다양한 논의를 이어나갈 것이다.
                </p>
            </Text>
        </Col>,
        date: "March 8"  
    },
    {
        title: "The bias of A.I",
        description: 
            <Col>
                <Row>
                    <Image width="500px;" src="https://www.nist.gov/sites/default/files/images/2022/03/14/22ITL003_risk-ai-final.jpg" />
                    <Col margin="margin-left: 15px;">
                        <Text className='text'>
                            <p>
                                인공지능을 개발하는데 현재 보편적인 방법은 바로 기계학습이다. Data를 Feeding함으로써 얻어지는 모델들은, 철저하게 데이터의 속성을 잘 계승 할 수 밖에 없다. 하지만 데이터는 그 자체로 완벽하게 특정 분야 혹은 집단의 속성을 계승하지는 못하고, 때로는 잘못된 데이터도 섞여 있다는 문제가 있다. 또한 모델을 어떻게 설계하냐에 따라서, 혹은 데이터의 부족 혹은 훈련과정의 잘못으로 모델이 데이터를 잘 흡수하지 못하는 등의 문제로 우리의 모델은 특정 학습되는 데이터 도메인을 잘 표현하지 못하거나, 잘못표현하거나, 더 나아가면 사회, 윤리적으로 합의 받지 못한 나쁜 속성을 계승하게 된다. 이러한 전체적인 문제를 바로 모델의 편향이라고 하며, 이는 기존에도 인공지능 개발에 있어 큰 난관이며, 실제적인 이용측면에서도 불편을 야기한다. 또한 인공지능이 인간사회로 진출할 수록 더욱 더 큰 '윤리적'인 문제로 부상할 수 밖에 없을 것이다.
                            </p>
                            <p>
                                이러한 AI 편향성은 크게 3가지 범주로 나눌 수 있는데, 이는 바로 알고리즘 편견, 부정적 계승, 과소적합이다. 알고리즘 편견은 보호되는 특징과 의사 결정에 사용되는 다른 정보 사이에 통계적 종속성이 존재하는 경우 발생한다. 부정적 계승은 AI 모델을 학습시키는 데 사용되는 데이터에 이미 존재하는 편향성을 나타낸다. 과소적합은 모집합에서 특정 구간에 대한 데이터가 충분하지 않아 모델이 확실한 결론을 내리지 못하는 경우다. 
                            </p>
                            <p><strong>각 범주를 더 세부적으로 살펴보면 다음과 같다.</strong></p>
                        </Text>
                    </Col>
                </Row>
                <Text className="text">
                    <ul>
                        <li>
                            <p><strong>알고리즘 편향</strong></p>
                            <p>
                                알고리즘 편견은 보호되는 특징과 다른 인자 간의 상관관계에서 비롯된다. 이 경우 보호되는 특성을 단순히 분석에서 배제하는 방법으로는 편향성을 줄일 수 없다. 이 상관관계가 보호되지 않는 인자를 근거로 해서 편향된 의사 결정으로 이어질 수 있기 때문이다. 
                                예를 들어 초창기 예측 치안 알고리즘은 예측을 수행할 때는 인종 데이터에 접근할 수 없었지만, 모델은 지리적 데이터(예를 들어 우편번호)에 크게 의존했고 지리적 데이터는 인종과 연관된다. 결국 성별, 인종과 같은 인구통계학적 데이터를 “볼 수 없는” 모델이라 해도 보호되는 특성과 통계적으로 연관된 다른 특징을 통해 여전히 이 정보를 코드화할 수 있다. 
                                대출 기관이 공정 대출법을 준수하는지 여부를 감시하는 미국 소비자 금융 보호국은 지리적 정보와 이름의 성 정보를 결합해 신뢰성 높은 인종 및 민족성 대리 확률(proxy probability)을 얻는 통계적인 방법을 발견했다. 알고리즘에 보호되는 클래스에 대한 접근 권한을 부여하지 않으면, 편향성이 자동으로 낮아진다는 기존의 오해를 뒤집는 발견이다. 대리 차별(proxy discrimination)로 불리는 이 현상은 근본적인 원인을 찾으면 완화할 수 있다. 즉, 대리 특징을 생성하는 모델 내의 중간 계산을 찾아 보호되는 특성과의 상관관계가 약한 값으로 대체하는 방법으로 교정할 수 있다. 
                                직관에는 반하지만, 모델 학습에서 보호되는 특징을 제거하는 순진한 해결책은 특정 사례에서 이미 불이익을 받고 있는 그룹에 오히려 피해를 줄 수 있다. 예를 들어 미국 사법 체계에서 교정 기관과 가석방 심의 위원회는 수감과 석방에 관한 공정한 의사 결정을 내리기 위해 위험 인자 체크리스트를 사용한다. 사람과 AI 모델이 모두 성별, 나이, 죄명, 성년 및 미성년 전과 횟수와 같은 정보를 가진 경우 내리는 결론도 비슷하다.  
                                그러나 사람과 모델에 교육 및 약물 사용에 관련된 10가지의 추가 위험 인자를 제공한 결과, 연구원들은 머신러닝 모델이 더 정확하고 편향성도 덜 갖는다는 사실을 발견했다. 이는 무턱대고 아무 교정 방법이나 도입해서는 안 되고 AI 모델의 편향성의 근본 원인을 파악해야 함을 의미한다. 
                            </p>
                        </li>
                        <li>
                            <p><strong>부정적 계승</strong></p>
                            <p>
                                학습 데이터에 존재하는 유사한 편향성에서 직접적으로 알고리즘 편향성이 발생할 수도 있다. 예를 들어 언어 번역 작업을 수행하도록 학습되는 ML 모델은 여성 이름을 ‘부모’, ‘결혼’과 같은 속성에 연결하는 경향이 있는 반면, 남성 이름은 ‘전문직’, ‘급여’와 같은 단어와 연결하는 경향이 강하다. 모델 스스로 이와 같은 연결 성향을 갖게 될 가능성은 낮다. 그보다는 이러한 성별 수사를 반영하는 말뭉치를 통해 학습되었을 가능성이 높다. 이것이 부정적 유산의 예다. 
                                자연어 처리에서 성별 편향성은 까다롭긴 해도 많은 연구가 이뤄진 문제다. 원인을 명확하게 파악하면 교정이 가능하다. 연구자들은 명사와 형용사에 대체로 남녀 구분이 없는 영어와 같은 언어에서 워드 임베딩(word embeddings)을 강제해서 성별 중립을 유지하는 방법을 발견했다. 본질적으로 성별을 구분하는 언어의 경우 성별 구분 단어와 성별 중립적 단어 간의 원인 연관성을 해체하는 새로운 예제를 수용하는 방법으로 언어 말뭉치를 보강해서 편향성을 차단할 수 있다. 
                                언어 외의 응용 분야에서 부정적 유산은 완화하기가 가장 어려운 편향성 유형이 될 수 있다. 머신러닝 모델이 학습하는 데이터세트에 본질적으로 편향성이 내제되기 때문이다. 이 경우 특정 인구에 대한 장기간의 제도적 편향성이 모델에 뚜렷하게 반영될 수 있다. 예를 들어 거주 지역을 근거로 일률적으로 대출을 거부하는 레드라이닝(redlining)으로 인해 대출 승인 데이터세트가 백인 쪽으로 편향될 수 있다. 데이터의 이 편향성은 AI 모델의 편향된 행동으로 이어진다. 
                                기존 편향성 완화 방법 중에서는 흑인 신청자의 신용 허용률을 높이는 방법도 있지만, 이 경우 모델 편향성의 진정한 원인이 가려져 문제를 해결하기가 어렵게 될 수도 있다. 신용 평가의 입력으로 흔히 사용되는 FICO 점수에도 전부터 인종 차별 경향이 드러났다. 이 경우 사후적 편향성 완화 방법은 신용도에 대한 원인 연관성을 가진 대체 데이터 소스를 찾는 방법보다 효과가 떨어질 수 있다. 부정적 유산은 대체 데이터를 찾는 방법으로 완화할 수 있다. 
                            </p>
                        </li>
                        <li>
                            <p><strong>Underfitting</strong></p>
                            <p>
                                데이터가 편향되는 경우도 있지만 부족한 경우도 있다. 머신러닝 모델은 충분한 데이터가 없으면 신뢰할 수 있는 예측을 제공하지 못한다. 이것이 과소평가 문제다. 아마존은 최근 채용 프로세스에서 지원자를 심사하기 위한 머신러닝 모델을 학습시켰지만, 다른 많은 IT 기업과 마찬가지로 아마존 역시 인력이 남성에 편중돼 있다. 이 데이터 불균형으로 인해 AI 모델은 남성 지원자를 평가할 때 더 확신을 갖게 되었고, 결과적으로 남성 지원자를 더 적극적으로 추천했다. 이 모델의 추천에서 편향성을 인지한 아마존은 채용 과정에서 해당 모델을 뺐다. 
                                아마존이 더 나은 데이터를 찾았다면 편향성 없는 채용 툴을 구축할 수도 있었을 것이다. 그러나 편향성이 발생하는 이유에 대한 제대로 된 이해가 없으면 해결은 불가능하다. 과소평가의 경우 여러 인구 하위 그룹에 걸쳐 모델의 예측 확실성을 분석하고, 기반 데이터세트를 새로운 인스턴스로 자동으로 보강하여 다양화할 수 있다. 
                                인구통계에 관한 모델 확실성과 안정성 척도는 모델이 모든 사람 그룹에 대해 신뢰할 수 있는 예측을 수행할 준비가 되었는지 여부를 파악하는 데 있어 매우 중요하다. 과소평가의 경우 제공되는 데이터세트가 데이터의 미세한 의미를 포착할 만큼 충분하지 않은 것이다. 그러나 공정성을 높이기 위한 적대적 학습 기법 또는 사후적 편향성 완화 방법은 데이터세트를 더 포괄적으로 보강하는 방법에 비해 성공 가능성이 낮다. 
                            </p>
                        </li>
                        <p><strong>결론</strong></p>
                        <p>
                            알고리즘이 편향성을 코드화, 영구화할 수 있다는 것은 주지의 사실이며, 이는 심각한 결과로 이어질 수 있다. 그러나 중요한 것은 알고리즘 편향성은 인간의 편향성과 달리 적절히 대처하면 최종적으로 수치화해서 교정할 수 있다는 점이다. 안전하고 신뢰할 수 있는 AI를 구축하기 위해서는 AI 편향성을 낮추기 위한 막무가내식 접근이 아니라 편향성 뒤의 진정한 원인에 대한 명확한 이해가 필수적이다. 
                            이러한 원인은 복잡하지만 연구자들은 특정 그룹에 대한 이질적 결과를 측정하고 차이를 유발하는 특징을 식별하고 편향성 원인별로 적절한 완화 방법을 선택하는 방법을 지속적으로 개발하고 있다. 자동화되는 의사 결정이 많아질수록 공정하고 공평한 모델을 만들기 위해서는 근본적인 편향성에 대처해야 한다.
                        </p>
                    </ul>
                </Text>
            </Col>,
        date: "March 10" 
    },
    {
        title: "The history of A.I. And about the state of the arts ",
        description: 
        <Col>
            <Image src="https://connectjaya.com/wp-content/uploads/2021/05/time.png" />
            <Text className='text'>
                <p><strong>인공지능의 탄생: 1943년 ~ 1956년</strong></p>

                1940년대 후반과 1950년대 초반에 이르러서 수학, 철학, 공학, 경제 등 다양한 영역의 과학자들에게서 인공적인 두뇌의 가능성이 논의되었고, 1956년에 이르러서 인공지능이 학문 분야로 들어섰다.
                1943년: 워렌 맥클록과 월터 피츠가 전기 스위치처럼 켜고 끄는 기초 기능의 인공 신경을 그물망 형태로 연결하면 사람 뇌에서 동작하는 아주 간단한 기능을 흉내 낼 수 있음을 증명했다.
                1950년: 엘론 튜링이 기계가 인간과 얼마나 비슷하게 대화할 수 있는지를 기준으로 기계 지능을 판별하는 튜링 테스트(Turing Test)를 제안했다. 튜링 테스트는 인공지능과 대화를 해서 그 반응을 인간과 구분하기 힘들다면 기계 역시 인간과 마찬가지로 지능적이라는 것을 주장하는 것이다. 튜링 테스트는 인공 지능에 대한 최초의 심도 깊은 철학적 제안이다.
                1951년: 맨체스터 대학의 페란티 마크 1(Ferranti Mark 1) 기계를 사용하여 크리스토퍼 스트레이(Christopher Strachey)는 체커 프로그램을 작성했고, 디트리히 프린츠(Dietrich Prinz)는 체스 프로그램을 작성했다. 아서 새뮤얼(Arthur Samuel)이 50년대 중반과 60년대 초반에 개발한 체커 프로그램은 결국 존경받는 아마추어에 도전할 수 있는 충분한 기술적 발전을 이룩했다. 게임 인공지능은 역사 속에서 인공 지능의 발전의 척도로 계속 사용될 것이다.

                 
                <p><strong>태동기: 1956년 ~ 1974년</strong></p>

                디지털 컴퓨터에 접할 수 있어진 50년대 중반에 이르러서, 몇몇 과학자들은 직관적으로 기계가 수를 다루듯 기호를 다루고, 사람처럼 기호의 본질적인 부분'까지 다룰 수 있을 것이라고 생각했다. 이것은 생각하는 기계를 만드는 새로운 접근 방법이었다.
                1956년: 미국 다트머스 대학에 있던 존 매카시 교수가 개최한 다트머스 회의에서 10명의 과학자가 모여 인공지능 개념을 정의했고, 인공지능 용어를 처음으로 사용했다. 즉, 인공지능이라는 용어가 생기기 전부터 인공지능은 연구되고 있었다. 당시의 인공지능 연구 핵심은 추론과 탐색이었다. 1956년 다트머스 컨퍼런스는 AI라는 이름, 목표점, 첫번째 성공과 이를 이룬 사람들, 그리고 넓은 의미의 AI 탄생을 포함하는 순간이었다. 다트머스 컨퍼런스 이후에, AI라는 새로운 영역은 발전의 땅을 질주하기 시작했다. 이 기간에 만들어진 프로그램은 많은 사람들을 놀랍게 만들었는데, 프로그램은 대수학 문데를 풀었고, 기하학의 정리를 증명했으며 영어를 학습했다. 몇 사람들은 이와 같은 기계의 지능적 행동을 보고 AI로 모든 것이 가능할 것이라 믿었다. 연구가들은 개인의 의견 또는 출판물들을 통해 낙관론을 펼쳤고, 완전한 지능을 갖춘 기계가 20년 안에 탄생할 것이라고 예측했다. 그리고 ARPA(Advanced Research Projects Agency) 같은 정부 기관은 이 새로운 분야에 돈을 쏟아부었다.
                1958년: 프랑크 로젠블럿이 인간의 뇌신경을 묘사한 인공신경 뉴런 '퍼셉트론(Perceptron)'을 제시했다.
                1965년: 최초의 전문가 시스템(Expert System)인 '댄드랄(DENDRAL)'이 개발되었다. - 전문가가 하던 일들을 시스템화한 것
                1969년: 민스키와 페퍼트가 '퍼셉트론즈'라는 책을 출판했다. 그리고 여기서 퍼셉트론의 결정적 문제점이 노출되어 신경망 연구 침체기 가 시작되었고, 연결주의가 추락했다. 여기서의 결정적 문제점은 A라는 개념과 B라는 개념을 정확히 분리해낼 수 있는 선형 분리 방식을 퍼셉트론이 적용하지 못하는 분야를 찾아낸 것이다. 따라서 퍼셉트론은 완벽하지 못한 AI이기 때문에 인공지능으로써의 효용성이 없다는 것을 증명하게 되었다.

                 
                <p><strong>첫 번째 암흑기 1974년 ~ 1980년</strong></p>

                1974년 무렵: 큰 기대를 가져왔지만, 인공지능 연구로 기대했던 결과를 보여주지 못했기 때문에 상당한 어려움에 봉착하게 되었다. 대규모 투자가 중단되었고, 기존 연구 프로젝트들이 줄줄이 취소되었다. 그리고 이때부터 전문가 시스템으로 연구 방향을 전환하게 되었다. (기호 주의의 부상) 전문가 시스템은 전문가의 지식을 논리적인 규칙으로 생성하여 특정 영역에 대해서 사람의 질문에 답할 수 있는 인공지능이다. 단, 전문가 시스템에는 적용할 수 있는 영역이 상당히 제한적이었으며, 사람이 규칙을 일일이 생성하고 이를 조합하여 논리적인 결과를 얻어내야 하는데 그 과정이 너무나 복잡했으며, 새로운 사실들을 추가하거나 문제점을 수정하는 등의 유지보수도 매우 어려웠다.

                 
                <p><strong>발전기 1980년 ~ 1987년</strong></p>

                1980년대: 인공지능계 최대 최대 화두는 신경망의 부활이었다. 사라진 단층 퍼셉트론 모델이 다층 퍼셉트론(신경망이 레이어드된 형태)으로 화려하게 컴백할 수 있었다. 다층 퍼셉트론에 쓰이는 역전파 알고리즘(오차를 줄일 수 있음)을 제안했고, 신경망은 패턴인식을 통해 문자, 인식, 영상 등의 인식에 크게 기여했다. 하지만, 데이터의 집합이 크고 복잡한 패턴을 처리하기 위해서는 히든 레이어를 여러 개 연결해야 하는데 오류역전파 방법으로는 제대로 학습이 되지 않았다. 그래서 신경망을 적용할 수 있는 범위가 한정될 수밖에 없었다.

                 
                <p><strong>두 번째 암흑기 1987년 ~ 1993년</strong></p>

                AI와 비즈니스 커뮤니티의 매력은 상실했고 경제 거품이라는 고전적 형태의 1980년대에 빠졌다. 붕괴는 정부기관과 투자자들의 ‘해당 분야는 계속해서 비판에도 불구하고 진보해왔다.’는 인식에 비롯된 것이었다.
                1987년대: 다층 신경망의 제한적 성능과 느린 컴퓨터의 속도로 복잡한 계산이 필요한 신경망 연구가 정체되었다. 적층을 해서 신경망을 늘려도 컴퓨터가 느리기 때문에 이를 해결할 수 없게 되었다. 따라서 미국방성 등 인공지능 연구 기금이 대폭 축소되었고 인공지능 연구는 두 번째 암흑기를 맞게 되었다. 이때 미국에서는 300개 이상의 상업용 인공지능 관련 회사가 사라지게 되었다.

                <p><strong>안정기 1993년 ~ 2011년</strong></p>

                1990년대 후반: 검색 엔진을 통해 이전과는 비교도 할 수 없이 방대한 데이터를 수집할 수 있게 되었고, 수많은 빅데이터를 분석하여 인공지능 시스템 자신 스스로 학습하는 머신러닝 형태로 진화하게 되면서 인터넷과 함께 인공지능은 중흥기를 맞이하게 되었다.
                1997년: IBM의 딥 블루(Deep Blue)가 체스 세계 챔피언이었던 가리 카스파로프를 6번의 대국 끝에 이겼다. 인간은 보통 10수 앞을 내다볼 수 있는데, 슈퍼컴퓨터였던 딥 블루는 12수까지 계산을 할 수 있었다.
                2004년: 제프리 힌튼 교수가 딥러닝 기반의 학습 알고리즘 제안(RBM) 아주 많은 층을 적층하고 결과물을 향상 시켰다. 불가능이라 여겨졌던 비지도 학습방법이 가능해졌고, 몇 분야에서는 인간의 수준을 뛰어넘은 결과물이 속속 나타나게 되었다. 딥러닝 알고리즘은 주로 음성 인식, 영상 이해, 기계 번역 등에 쓰이고 있다.
                2011년: IBM의 왓슨이 TV 퀴즈 쇼에서 인간 우승자들에게 승리했다. 사람들의 질문을 이해하고 맥락을 이해해서 답변을 했다.

                 
                <p><strong>부흥기 2011년 ~ 현재</strong></p>

                2012년: 구글이 심층신경망(DNN)을 구현하여 고양이 영상인식을 성공시켰다. (1만 6천 개의 컴퓨터로 무려 10억 개 이상의 신경망을 구성)
                2014년: 페이스북에서 '딥페이스'라는 얼굴인식 알고리즘을 개발했다. 97%의 성능으로 사람 얼굴을 구분할 수 있다.
                2016년: 구글 딥마인드의 알파고(AlphaGo)가 이세돌 9단과의 바둑대결에서 승리했다. 알파고는 지도학습과 비지도 학습을 동시에 사용하는데, 비지도 학습의 한 종류인 강화 학습 기술로 혼자 대국을 두며 학습할 수 있었다. 딥블루가 체스에서 우승한 것과 비슷한 사례로 보일 수 있지만, 딥블루는 규칙에 따라 행동하기 때문에 체스 말고는 다른 일을 할 수 없다. 그러나 알파고는 사람처럼 학습을 통해 배울 수 있는 능력이 있기 때문에 딥러닝 알고리즘을 사용하면 바둑뿐만 아니라 다른 분야에서도 그대로 적용이 가능하기 때문에 응용할 수 있는 범위가 무궁무진하다.

                 
                인공지능의 부흥기는 현재 진행형이다. 그리고 최근에 들어 관련된 연구가 급증하고 있는데, 이에 관한 이유는 3가지로 정리해볼 수 있다.

                데이터: 2025년까지 163 제타바이트(1 테라의 10억 배)의 데이터가 생성될 것이라고 IDC에서 발표했다.
                알고리즘: 딥러닝 등 최신 알고리즘이 개발되고 계속해서 성능이 발전하고 있다.
                컴퓨팅 파워: 2018년 출시된 GPU(병렬처리가 가능)는 5년 전 출시된 가장 빠른 버전보다 40~80배 빠르다. 비단 알파고 뿐만 아니라 자율 주행 자동차까지, GPU는 인공지능 시스템의 발달에 있어서 가장 선두를 달리고 있다.
            </Text>
        </Col>,
        
        
        //"1. 역사 (겨울) 2. and let's brifely know about how the ai is different between other algorithms (Problem Solving, Reasoning, Learning, Regcognition지식 베이스 등등)/ super intelligence 까지",
        date: "March 15",
    },
    {
        title: "인간과 기계의 지능",
        // 16 - 44
        description: <Col>
            <Text className='text'>
                <p>
                    인간이 자신을 Homo sapiens - 현명한 사람 - 라고 부르는 이유는 정신적인 능력이 인간에게는 매우 중요하기 때문이다. 수천년동안 '인간은 어떻게 생각하는가' ; 즉 어떻게 한줌밖에 안되는 인간이 그 자신보다 훨씬 더 크고 복잡한 세계를 감지하고 이해하고 예상하고 조작하는지를 이해하기 위해 노력해왔다. 인공지능 분야, 즉 AI 는 단지 이해 하는데 그치지 않고 지능적인 실체를 만들려고 시도한다.
                </p>
                <p>
                    최근의 AI 는 많은 다양한 하위분야를 포함하는데, 예를 들면 학습 (Learning) 과 지각 (Perception) 같은 일반적인 목적의 영역으로부터 체스 (Chess) 게임, 수학의 정리증명 (Theorem Proving), 시 (poetry) 쓰기, 질병의 진단과 같은 특수영역까지를 포함한다. AI 는 지능적인 작업들을 시스템화하고 자동화하며, 따라서 아마도 인간 지능활동의 일부영역에 상응할 것이다. 이러한 의미에서 AI 는 진실로 범용 (universal) 분야이다.
                </p>
                <p>
                    우리는 인공지능 (Artificial Intelligence) 에 열광하면서도 그것이 무엇인지 정의하지는 않았다. 8 개의 텍스트에 묘사된 AI 의 정의를 그림 1.1 에 보여준다. 이러한 정의는 크게 두가지 차원에서 다른다. 대강, 그림의 위쪽에 있는 것은 사고 (thought) 과정과 추론과 관련된 것이고, 아래에 있는 것은 행동 (behavior) 에 관한 것이다. 왼쪽에 있는 것은 인간의 (human) 의 능력에 충실하다는 의미에서의 성공을 다루고, 오른쪽은 우리가 합리성 (rationality) 라고 부르는 지능의 이상적인 (ideal) 개념을 다룬다. 어떤 시스템이 자신의 지식으로 만일 "정확한 일 (right thing)" 을 한다면 그 시스템은 합리적 (rational) 이다.
                </p>
                <p>
                    역사적으로 모두 4 가지의 AI 의 접근방식 (approaches) 이 있어왔다. 예상한 대로, 인간을 중심으로 한 접근방식과 합리성을 중심으로한 접근방식 사이에 긴장이 존재한다 (주석 : human and rational behavior 를 구분할 때, "감정적으로 불안정한" 또는 "제정신이 아닌 (insane)" 이라는 의미로 인간은 반드시 "비합리적 (irrational)" 이라고 전제하는 것이 아니란 것을 지적해야 한다. 단지 인간은 완전하지 않다는 개념을 필요로 한 것이다 : 체스의 규칙을 모두 알더라도 인간이 모두 체스의 고수인 것은 아니다 ; 그리고 불행히도, 모든 사람이 시험에서 A 학점을 받는 것은 아니다. 인간의 추론에서의 시스템적인 오류 (errors) 에 대해서는 Kahneman et al (1982) 에 분류되어 있다). 인간 중심의 접근방식은 가설과 실험적인 확인을 포함하는 경험적 과학이어야만 한다. 합리주의자의 접근방식은 수학과 공학의 조화를 포함한다. 각 그룹은 서로를 비난하면서도 서로에게 도움을 준다. 4 개의 접근방식을 더 자세히 보자.
                </p>
                <Col style={{border: 'solid 1px white'}}>
                    <Row align="center" justify="space-around" style={{border: 'solid 1px white'}}>
                        <p>
                            <strong>인간처럼 생각하는 시스템</strong>
                        </p>
                        <p>
                            <strong>합리적으로 생각하는 시스템</strong>
                        </p>
                        <p>
                            <strong>인간처럼 행동하는 시스템</strong>
                        </p>
                        <p>
                            <strong>합리적으로 행동하는 시스템</strong>
                        </p>
                    </Row>
                    <Row justify="space-around">
                        <Col style={{border: 'solid 1px white'}}>
                            <p>
                                "컴퓨터를 생각하게 만들기 위한 재미있는 새로운 노력 ....  말 그대로 '마음을 가진 기계' " (Haugeland 1985)
                            </p>
                            <p>
                                "의사결정, 문제해결과 같은 활동, 즉 인간의 사고 (thinking) 와 관련된 활동의 자동화 ..... " (Bellman 1978)
                            </p>
                        </Col>
                        <Col style={{border: 'solid 1px white'}}>
                            <p>
                                "계산적 모델 (computational models) 의 사용을 통한 정신적 능력 (mental faculties) 에 대한 연구" (Charniak and McDermott 1985)
                            </p>
                            <p>
                                "지각, 추론, 행동을 가능하게 하는 계산들 (computations) 에 대한 연구" (Winston 1992)
                            </p>
                        </Col>
                        <Col style={{border: 'solid 1px white'}}>
                            <p>
                                "인간이 하면 지능을 필요로 하는 그러한 기능 (function) 을 수행하는 기계를 창조하는 기술" (Kurzweil 1990)
                            </p>
                            <p>
                                "인간이 더 잘하는 것을 (things) 어떻게 하면 컴퓨터가 하게 만들지를 연구하는 것" (Rich and Knight 1991)
                            </p>
                        </Col>
                        <Col style={{border: 'solid 1px white'}}>
                            <p>
                                "계산 지능 (Computational Intelligence) 은 지능적 에이전트를 설계하는 것에 대한 연구이다." (Pool et al 1998)
                            </p>
                            <p>
                                "AI 는 ......인공물에서의 지능적 행동 (behavior) 과 관련된다." (Nilsson 1998)
                            </p>
                        </Col>
                    </Row>
                </Col>
            </Text>
        </Col>,
        date: "March 17",
        part: "Stuart Russell - Human-Compatible",
        chap: "Chap 2. Intelligence in humans and machines",
    },
    {
        title: "인간과 기계의 지능",
        // 30 - 69
        description: 
            <Col>
                <Text className="text">
                    <p><strong>인간처럼 행동하기 : Turing Test 접근방식</strong></p>
                    <p>
                    Alan Turing (1950) 이 제안한 튜링 테스트 (Turing Test) 는 지능의 만족스런 작동을 정의하기 위해 설계되었다. 지능을 위해 요구되는 자격에 대해 논쟁거리를 제안하는 것보다는, 그는 부정할 수 없는 지능적인 개체인 인간 (human being) 과 구분할 수 없음 (indistinguishability) 에 바탕한 하나의 테스트를 제안했다. 만일 인간 질문자가 문서화된 질문을 하였을 때 마찬가지로 문서화된 반응이 사람이 한것인지 아닌지를 말할 수 없다면 그 컴퓨터는 테스트를 통과한 것이다. 컴퓨터는 다음의 능력을 가질 필요가 있는 것이다 :

                    자연어처리 (Natural Language Processing) 는 컴퓨터가 영어로 대화하는 것을 가능하게 한다.

                    지식 표현 (Knowledge Representation) 은 컴퓨터가 아는 것 또는 들은 것을 저장한다.

                    자동 추론 (automated reasoning) 은 질문에 답하기 위해서 그리고 새로운 결론을 유도하기 위해서 저장된 정보를 사용한다.

                    기계 학습 (Machine Learning) 은 새로운 환경에 적응하고 패턴들을 감지하고 추정한다.

                    Turing Test 는 질문자와 컴퓨터 간의 직접적인 물리적 상호작용은 의도적으로 피했다, 왜냐하면 사람의 물리적인 (physical) 시뮬레이션은 지능에는 불필요하기 때문이다. 그러나 소위 total Turing Test 는 질문자가 "칸막이를 통한 (through the hatch)" 질문 뿐만 아니라 컴퓨터의 지각 (perceptual) 능력을 테스트 할 수 있도록 비디오 신호를 포함한다. total Turing Test 를 통과하기 위해서는 컴퓨터는 다음의 기능을 필요로 한다.

                    컴퓨터비전 (Computer Vision) 은 물체를 지각한다.

                    로봇 (Robotics) 은 물체를 조작하고 움직인다.

                    이러한 6 개의 과목이 AI 의 대부분을 구성하며, Turing 은 그후 50 년간이나 정당한 것으로 여전히 평가받는 테스트를 설계한 명성을 인정받아 마땅하다. 그러나 AI 연구자들은 Turing Test 에 통과하려는 노력을 거의 할 수가 없었는데, 그것은 원본을 복제하는 것보다는 지능에 잠재해 있는 원리에 대한 연구가 더 중요하다고 믿었기 때문이다. "인공적인 비행체" 에 대한 연구는 Wright 형제와 다른 연구자들이 새를 모방하는 것을 멈추고 항공역학 (aerodynamics) 을 배우면서 성공하였다. 항공공학 텍스트는 그 분야의 목표를 "그들이 비둘기처럼 어리석게 보일 수 있는, 비둘기처럼 정확히 똑같게 날 수 있는 기계" 를 만드는 것으로 정의하지 않는다.
                    </p>
                    <p><strong>인간처럼 생각하기 : 인지 모델 (cognitive modeling) 접근방식</strong></p>
                    <p>어떤 프로그램이 인간처럼 생각하는 것을 언급할 때, 인간이 어떻게 생각하는 지를 파악해야만 한다. 즉 인간의 마음의 실제적인 작업을 이해할 (get inside) 필요가 있다. 그를 위해 두가지 방법이 있다 : 자기성찰 (introspection) - 스스로의 사고 (thought) 가 어디로 움직이는 지를 포착하기 위한 노력 - 을 통한 방법과 심리학적 실험을 통한 방법이다. 만일 프로그램의 입력/출력 과 시간에 따른 행동이 인간의 행동과 같은 것으로 매치가 된다면, 그것은 그 프로그램의 일부 메카니즘이 인간에서도 작동될 수 있다는 증거이다. 예를들면, GPS, "General Problem Solver" (Newell and Simon 1961) 을 개발한 Allen Newell 과 Herbert Simon 은 그들의 프로그램이 문제들을 정확하게 풀기위한 내용은 아니었다. 그들은 프로그램의 추론 단계의 궤적과 인간이 같은 문제를 풀어나가는 궤적을 비교하는 것에 더 관심을 가지고 있었다. 학제간 분야인 인지과학 (Cognitive Science) 은 AI 로부터 컴퓨터 모델, 심리학에서 실험 기술을 받아들여 인간 마음의 작동에 대한 정확하고 테스트 가능한 이론을 만들려고 시도한다.

                    인지과학은 그 자체가 백과사전과 같은 가치가 있는 매력적인 분야이다 (Wilson and Keil 1999). 이 책에서는 인간의 인지 (Cognition) 에 대해 무엇이 알려져 있는지를 묘사하려고 하지는 않을 것이다. 우리는 가끔 AI 기술과 인간의 인지사이의 유사성과 차이점에 대해 언급한다. 그러나 실질적인 인지과학은 반드시 실제의 인간이나 동물에 대한 실험적인 조사에 기초해야만 하고, 당연히 독자들은 실험을 위한 컴퓨터에만 접근하다.

                    AI 의 초기에는 접근방법 (approaches) 간에 혼란이 있기도 했다 : 어떤 알고리즘이 작업을 잘 수행하고 그럼으로써 (therefore) 그것은 인간의 수행을 잘 모델링 한 것이라고 주장하였고, 그 역으로 주장하기도 하였다. 요즘에는 그 두가지 종류의 주장을 분리한다 ; 이러한 구별은 AI 와 인지과학이 둘다 급격하게 발달하게 하였다. 두 분야는 서로를 더욱 기름지게 하였는데, 특히 시각 (vision) 과 자연어 (natural language) 영역이 그렇다. 특히 vision 은 신경생리학적 증거와 컴퓨터 모델을 통합한 접근방식을 통해서 최근에 많이 발전하였다.  
                    </p>
                    <p><strong>합리적으로 생각하기 : 사고의 법칙 (laws of thought) 접근방식</strong></p>
                    <p>그리스 철학자 Aristotle 은 반박할 수 없는 추론과정으로서의 "정확한 생각 (right thinking)" 을 최초로 체계적으로 정리하였다. 그의 삼단논법 (syllogism) 은 정확한 전제가 주어졌을 때 항상 정확한 결론을 낳는 - 예를들면 "소크라테스는 사람이다 ; 모든 사람은 죽는다 ; 따라서 소크라테스는 죽는다" - 추론구조의 패턴을 제공했다. 이러한 사고의 법칙이 마음의 동작을 지배하는 것으로 가정했다 ; 그들의 연구는 논리학 (Logic) 이라는 분야를 만들었다.
                    19 세기의 논리학자들은 세상의 모든 종류의 사물들과 그것들 간의 관계에 대한 문장의 정확한 표기법 (notation) 을 개발했다. (이것을 수에 관한 문장의 동등성 (equality and inequality) 여부를 주로 제공하는 산술 표기법과 비교해 보라). 1965 년까지, 원칙적으로 논리적 표기법으로 묘사되는 해결가능한 어떤 (any) 문제들도 프로그램으로 풀 수 있다는 믿음이 존재했다 (주석 : 만일 해법이 없다면 그 프로그램은 그것을 찾을 때까지 결코 멈추지 않을 것이다).  소위 AI 에서의 logicist 전통은 지능시스템을 창조하기 위해 그런 프로그램을 토대로 하기를 희망한다.

                    이러한 접근방식의 2 개의 큰 장애가 있다. 첫째로 비형식적인 (informal) 지식을 취해서 논리적 표기법에 필요한 형식적 (formal) 용어로 서술하기가 쉽지 않은데, 특히 지식이 100 % 확실하지 않을 때 그렇다. 둘째로 "원칙적으로" 문제를 해결할 수 있다는 것과 실제로 해결하는 것 사이에 큰 차이가 존재한다. 단지 수십개의 사실 (facts) 을 가진 문제들 조차, 최초의 추론 단계에서 가이드를 받지 못하면 컴퓨터의 계산 자원 (computational resources) 를 모두 써 버릴 수 있다. 이러한 장애들에도 불구하고 컴퓨터 추론시스템을 만드는 어떤 기도가 성공한다면, 그것은 logicist 전통에서 최초가 될 것이다.
                    </p>
                    <p><strong>합리적으로 행동하기 : 합리적인 에이전트 (rational agent) 접근방식 </strong></p>
                    <p>Agent 란 단지 행동하는 어떤 것이다 (agent 는 라틴어 agere (to do) 에서 나온 말이다). 그러나 컴퓨터 에이전트는 단순한 "프로그램" 이 아니라, 자율적 제어 하에서 작동하고, 주위 환경을 지각하며, 오랜 시간동안 살아남고 (persist over), 변화에 적응하며, 다른 에이전트의 목표를 흉내낼 수 (take on) 있는 속성을 가졌다는 면에서 독특한 것으로 생각된다. 합리적인 에이전트 (rational agent) 란 최상의 결과 (outcome), 불확실성이 있는 경우에는 최상의 기대 (expected) 결과를 얻을 수 있도록 행동하는 것이다.

                    AI 에 대한 "사고의 법칙" 접근방식은 정확한 추론 (correct inference) 을 강조했었다. 정확한 추론을 한다는 것은 때로는 rational agent 이기 위한 부분 (part) 이다. 왜냐하면 합리적으로 행동하는 하나의 방법은 (주어진 행동이 목표를 달성할 것이라는 결론에 따라) 논리적으로 추론하고  그리고나서 그 결론에 따라 행동하는 것이기 때문이다. 그와는 달리 정확한 추론이 합리성의 전부는 아닌데, 그 이유는 정확하다고 증명될 수는 없지만 계속 수행되어야만 하는 상황이 가끔 있기 때문이다. 또한 추론을 포함하지 않으면서도 합리적으로 행동하는 경우들이 있다. 예를들면, 뜨거운 난로에서 얼른 손을 떼는 것은 주의깊은 숙고를 한 후에 느리게 행동해서는 안되는 성공적인 반사 행동이다.

                    Turing Test 를 위해 필요한 모든 기술들은 합리적 행동들을 허용하는 것이다. 그래서 아주 다양한 상황에서 좋은 결정을 할 수 있어야 하기 때문에 지식을 표현하고 그 지식으로 추론하는 능력을 필요로 한다. 또한 자연어로 이해할 수 있는 문장들을 생성할 수 있는 능력을 필요로 하는데, 그것은 그 문장들이 복잡한 사회에서 그럭저럭 헤어날 수 있게 도와주는 말을 하기 때문이다. 우리는 단지 박학다식한 것을 요구하는 것이 아니라 학습 (Learning) 을 필요로 하는데, 세상이 어떻게 돌아가는지에 대해 더 잘 알게되면 세상을 다루는데 있어서 더 효율적인 전략을 생성할 수 있기 때문이다. 우리는 시각적 지각 (visual perception) 을 필요로 하는데, 본다는 것이 재미있기 때문이 아니라 어떤 행동을 취할 것인지를 더 잘 알 수 있기 때문이다 - 예를들면 맛있는 음식을 볼 수 있다는 것은 그 방향으로 움직이게 도와준다.

                    이러한 이유 때문에 rational-agent 설계로서의 AI 에 대한 연구는 적어도 2 가지 장점을 가지고 있다. 첫째로 그것은 "사고의 법칙" 접근방식 보다도 더 일반적인데, 그 이유는 정확한 추론은 합리성 (rationality) 을 얻기위해 가능한 몇 개의 메카니즘 중의 하나에 불과하기 때문이다. 둘째로 인간의 행동이나 인간의 사고에 기초한 접근방식보다 더 과학적 개발의 원리에 따르는 것인데, 그 이유는 합리성에 대한 표준은 명확하게 정의되고 완전히 일반적이기 때문이다. 그와는 달리, 인간의 행동은 특수한 환경에 잘 적응하고, 부분적으로는 복잡하고 잘 알려지지 않은 진화 과정 (완전한 것을 만들어 내는 것과는 거리가 먼) 의 산물이다.
                    </p>
                </Text>
            </Col>,
        date: "March 22, 29",
        part: "Stuart Russell - Human-Compatible",
        chap: "Chap 2. Intelligence in humans and machines",
    },
    {
        title: "앞으로 A.I는 어떻게 발전할까",
        // 95 - 150
        description: 
        <Col>
            <Text className="text">
                <Row>
                    <Image width="25%" src="https://americankahani.com/wp-content/uploads/2020/07/ai-image-1.jpg" />
                    <Col margin="margin-left: 15px;">
                        <p><strong>초지능, 특이점에 대해서</strong></p>
                        <p>인공지능이 발전할 수록 빠질 수 없는 이야기가 바로 초지능과 특이점이다. 우리는 인공지능 개발과 연구에 있어서 초지능과 특이점이 도래함은 명확히 알수 없지만, 기술의 발전은 매우 방대하고, 우리는 이를 마땅히 경계해야 한다는 것이다. 특이점의 사전적 정의는 다음과 같다.</p>
                        <p style={{fontStyle: 'italic'}}>
                            기술적 특이점(技術的特異點, 영어: technological singularity, TS)은 인공지능(AI)의 발전이 가속화되어 모든 인류의 지성을 합친 것보다 더 뛰어난 초인공지능이 출현하는 시점을 말한다. 즉, 특이점이란 미래학에서 문명의 미래 발전에 가상 지점을 뜻하는 용어로서, 미래에 기술 변화의 속도가 급속히 변함으로써 그 영향이 넓어져 인간의 생활이 되돌릴 수 없도록 변화되는 기점을 뜻한다. 미래연구에 있어서 인류의 기술 개발 역사로부터 추측하여 얻을 수 있는 미래 모델의 정확하고도 신뢰할 수 있는 한계인 「사상의 지평선」을 가리킨다.
                        </p>
                        <p style={{fontStyle: 'italic'}}>-Wikipedia</p>
                    </Col>
                </Row>
                <p>
                    러셀은 초지능의 출현에 대해서 말을 아끼지만 많은 시간이 지나서 구현할 수 있을 것이라는 태도를 취하고 있다. 특히 일반 인공지능을 구현하고 이를 현명하게 통제해 인간 지능을 강화할 수 있다면 우리가 접하고 있는 가난, 질병, 사회 문제를 해결할 수 있어 인류에게 큰 이익이 될 것이라고 얘기한다.
                    그러나 그냥 긍정적인 입장이 아니라 인간 사회에서 발생할 수 있는 오용의 문제, 인간을 능가하는 경우에 겪을 수 있는 실존의 문제(고릴라 문제), 불완전한 문제와 가치 정렬 실패(미다스 왕 문제), 도구적 목표와 인간 선호 간의 충돌 등 그가 여러 에세이에서 거론했던 문제를 이 책에서 다시 정리해서 설명하고 있다.
                </p>
                <p>
                현재 표준 모델로 볼 수 있는 인공지능에 목적 함수를 정하고 이 목적을 위한 수많은 데이터를 통해 학습을 시키는 방식에서 우리의 가치와 정렬하지 못한 목적이 사용되는 경우에서 현재 많은 문제가 발생하고 있다는 것이 그의 진단이다.
                    이 문제에 대한 그의 제안은 인공지능이 정해진 목적이 아닌 우리의 목적을 지속적으로 찾아내고 이를 통해 인공지능의 인센티브를 전환하게 만들자는 것이다. 이는 그가 지난 몇 년 동안 논문과 에세이를 통해 주장한 ‘역 강화학습(IRL)’ 방식을 의미한다.
                    특히 많은 사람이 존재하는 사회에서 인간이 갖는 선호를 어떻게 조율할 것인가 하는 문제에서 그는 공리주의 원칙을 강하게 주장한다. 특히 인공지능 연구자가 인간의 선호를 설정하는 일은 결코 있으면 안 된다고 말하면서, 우리가 흔히 얘기하는 단순한 공리주의가 아닌 좀 더 균형적이고 합리적인 방향을 위해 많은 철학자, 사회학자, 경제학자의 논의를 소개한다. 심지어 어벤저스의 타노스나 매트릭스까지 등장시키면서.
                    마지막으로 러셀 교수도 딥러닝의 한계를 지적하는데, ‘더욱 크고 더욱 깊은 망과 더욱 많은 데이터 집합과 더욱 큰 기계를 만든다고 해서 인간 수준의 AI가 나오지 않는다는 것이다’라면서 ‘더욱 고등한 사고와 상징 추론’이 필수적이라는 딥마인의 하사비스 견해를 소개한다. 이를 설명 기반 학습과 인지과학의 덩이짓기(chunking)이라는 유형의 학습이 필요하다는 것을 제시하면서 마무리 짓는다.
                </p>
            </Text>
        </Col>,
        date: "March 31",
        part: "Stuart Russell - Human-Compatible",
        chap: "Chap 3. How might AI progress in the future?",
    },
    {
        title: "A.I의 오용, 지나치게 지적인 A.I",
        // 156 - 166
        description: <Col>
            <Text className="text">
                <p><strong>1. 인간 윤리와 존엄성</strong></p>
                <p>
                    하지만 AI의 발전상에 대해 기대뿐만 아니라 부정적 영향에 대한 우려도 크다. 세계적 우주물리학자 스티븐 호킹은 2018년 타계하기 직전까지 “AI의 등장은 인류의 멸망을 초래할 수 있다”고 경고했다.
                    AI는 윤리와 존엄성의 문제를 제기한다. 인간은 고도의 이성과 판단력을 지녔기 때문에 주체성을 지닐 수 있다. AI 기술이 고도화되고 자동화 수준이 높아질수록 기게는 인격성을 갖고 위험의 책임 주체가 된다. 인간성이 존재하지 않는 AI 기기에 자율적 의사 결정 기능을 부여하면 통제할 수 없는 상황이나 예기치 못한 문제가 생길 수 있다.
                    만약 AI가 운전하는 자율주행자동차가 인명 사고를 내면 누가 책임을 져야 하는가. 갑자기 공을 잡기 위해 도로로 뛰어든 아이를 피하기 위해 자율주행자동차가 다른 사람이 있는 인도를 덮친다면 AI의 이런 판단은 존중받을 수 있을까. AI 투자 시스템이 잘못된 판단으로 고객의 자금을 모두 날렸다면 고객은 누구에게 보상을 청구해야 하는가. 미처 고려하지 못했던 조건이나 상황에 직면했을 때 AI가 제어할 수 없는 상황은 얼마든지 발생할 수 있다.
                    그중에서도 과학자들이 가장 우려하는 최악의 상황은 AI가 전쟁 무기로 악용되는 것이다. 자율살상무기시스템(LAWS, Lethal Autonomous Weapons Systems)이 인간의 통제를 벗어나 목표를 제거한다면 인간의 존엄성은 추락할 것이다. 기계가 프로그램상 오작동을 일으켜 무고한 시민들을 살상한다면 영화 ‘터미네이터’ 시리즈의 암울한 디스토피아가 현실이 되는 셈이다.
                </p>
                <p><strong>2. 프라이버시 개인 정보 침해</strong></p>
                <p>
                    AI는 프라이버시의 자유를 위협한다. 중국 정부는 범죄 용의자 추적을 목적으로 지하철, 공항 등 사람이 많이 다니는 곳에 CCTV 수억 개를 설치해 이로부터 수집되는 방대한 개인 얼굴과 동작 데이터를 AI 기술로 학습시켜 특정 개인의 위치와 상태를 감시하고 있다.
                    최근 AI 안면 인식 기술은 장거리에서 감정을 인식하는 수준까지 발전했다. 수백만 명이 모인 곳에서 특정 인상착의를 지닌 사람을 찾는 것은 물론 슬픔과 분노까지 인식해 자살 위험이 크거나 범죄를 저지를 가능성이 큰 사람, 은행에서 돈을 빌리고 갚지 않을 사람까지 판별할 수 있다. 이름이나 주소를 넘어 개인이 드러내고 싶지 않은 감정까지 개인 정보로 축적되는 것이다.
                    우리나라에서도 최근 코로나19 자가격리 위반자에게 위치 추적용 전자팔찌를 채우게 했다. 확진자의 휴대전화 및 신용카드 사용 기록을 조회해 동선을 공개하는 등 코로나19 확산을 틈타 개인정보가 남용되는 디지털 감시 사회에 가까워지고 있다. 감시와 통제가 사회 안전이나 방역에 도움을 준다고 해도 프라이버시와 개인 정보 침해라는 더 큰 피해를 낳을 수 있다.
                </p>
                <p><strong>3. 일자리 감소</strong></p>
                <p>
                    일자리 감소는 가장 피부로 와닿는 AI의 위협이다. 최근 공개된 정부 보고서에 따르면 AI 등 디지털 기술의 대체로 일자리를 잃을 사람이 향후 10년간 700만 명이라고 한다. 우리나라 노동자 2,300만 명 중 3분의 1에 해당하는 수치로서 광범위한 실업과 저임금 고통을 예고한다.
                    AI는 제조업과 서비스 업종은 물론 전문직까지 대체할 전망이다. 매뉴얼에 기반한 텔레마케터나 콜센터 상담원, 운송업자, 노동 생산직은 물론 기자, 법률 상담, 회계, 의료 등 전문 서비스 직종 직업군 역시 대체될 가능성이 큰 직업군으로 꼽힌다.
                    AI의 직격탄을 맞고 있는 대표적 분야가 금융업이다. 인터넷과 스마트폰에 의한 입출금 등의 업무 자동화에 이어 AI를 이용한 로보어드바이저(roboadviser:자동 온라인 금융 자문)가 대출 상담 업무까지 대신하면서 은행 창구 직원(텔러)과 오프라인 점포 수는 갈수록 줄어든다. 빅데이터를 활용해 소비자의 편의성을 극대화한 보험 서비스인 인슈어테크(insure tech)가 등장하여 보험 판매원은 물론 금융상품을 설계하는 고급 인력도 사라질 가능성이 크다.
                    “AI는 단순 반복적인 업무를 도맡아 효율성을 높일 뿐 고도의 분석력과 창조력, 감성이 필요한 업무는 인간의 영역으로 남을 것”이란 낙관론도 장담할 수 없다. 문학, 음악, 회화 등 인간 고유의 영역으로 여겼던 예술 분야에서도 AI가 두각을 나타내고 있다.
                    실제로 IBM의 AI 슈퍼컴퓨터 ‘왓슨’은 200만 건의 처방을 실수 없이 조제한 로봇 의사이자 약사로서 이미 국내 많은 병원에 도입돼 의료진의 일원으로 활용하고 있다. AI가 빈센트 반 고흐나 렘브란트의 화풍을 모사한 작품은 “예술인가 기술인가”라는 논란에도 고가에 팔린다.
                </p>
                <p><strong>4. 대응 방안</strong></p>
                <p>
                    AI의 통제 불능 및 악용으로 인간 존엄성이 위협받는 문제를 해결하기 위해서는 먼저 AI의 권한 설정과 결과에 대한 책임 소재를 명확히 할 필요가 있다. 인간과 AI의 공존을 고려한 새로운 윤리 규범 체계와 법제화의 정립이 요구된다.
                    인간의 개입 없이 AI가 윤리적 판단을 하는 주체가 되어서는 안 될 것이며 AI로부터 파생된 피해에 대해서는 이를 가능케 한 사람에게 책임을 물을 수 있어야 한다. 윤리적 가치판단은 사람이 내리고 AI는 개발 단계부터 사람을 돕는 기능적 역할을 담당해야 하는 데 그치도록 세심하게 설계돼야 한다.
                    유럽연합(EU)은 2017년 AI와 로봇 제작자의 윤리 규범, 책임 등이 포함된 로보틱스(로봇공학)에 관한 민법 결의안을 발표했다. 이듬해 시행된 개인정보보호법에 AI에 의한 자동화 의사결정 거부권인 프로파일링 거부권도 두었다. 일본 정부는 기업체가 개발하는 AI의 안정성과 보안성을 평가하는 공적 인증제도를 운용하기로 했다.
                    우리나라는 AI의 책임성과 윤리성을 다루는 관련법이 전무한 실정이다. 그나마 입법이 논의되는 부분도 AI 기술개발이나 산업 진흥책 부분에 한정되고 있다. AI의 장밋빛 미래만 기대하며 다가올 위협에는 눈을 감고 있는 것이다.
                    일자리 감소 우려와 관련해서도 대비가 필요하다. AI는 많은 직업을 도태시킬 것이지만 AI와 직간접적으로 연관해 새로운 일자리를 창출할 수 있다. 데이터 과학이나 로봇 연구, 소프트개발 운용, 수리 및 유지 보수 등의 시장 수요는 갈수록 증가할 것이다.
                    이러한 일자리 구조 변화에 대응하기 위해 인재를 양성해야 한다. AI 대체에 취약한 직무에 근무하는 사람들이 전문성을 가지고 적합한 역할을 할 수 있도록 교육 시스템을 개선하고 직종 간 이동이나 업무 변화에 적응할 수 있도록 재교육·훈련 프로그램을 확대할 필요가 있다.
                    미국 최대 유통 기업 아마존은 지난해 7억 달러(약 8,000억 원)를 투자해 직원 10만 명에게 재교육을 실시했다고 발표했다. 우리나라 정부와 기업도 노동 시장의 변화에 맞춰 직업 재교육 체계를 강화하고 고용 구조를 재편해야 한다. 이와 함께 앞으로 본격적인 AI 시대를 맞이하는 학생들의 교과 과정은 기술을 이해하고 통제하면서도 인간 고유의 감성과 공감 능력을 키울 수 있도록 설계돼야 한다.
                </p>
            </Text>
        </Col>,
        date: "April 5, 7",
        part: "Stuart Russell - Human-Compatible",
        chap: "Chap 4. Misuses of AI, Chap 5. Overly intelligent AI",
    },
    {
        title: "A.I: 다른 접근법",
        // 250 - 268
        description: <Row margin="margin-top: 15px;" align="center">
            <Flex flex={3}>
                <iframe width="750" height="500" src="https://www.youtube.com/embed/EBK-a94IFHY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </Flex>
            <Flex flex={3.2}>
                <Text className='text' style={{marginLeft: 15}}>
                    <p>
                        우리는 로봇의 지배를 받는 재앙을 막으면서도 초지능 인공지능과 함께 공존할 수 있을까?
                    </p>
                    <p>
                        스튜어트 러셀은 조금 다른 관점에서 상식적이고 이타적이며 인간 가치를 통해 인간과 공존 가능한 인공지능이 갖는 문제와 이를 해결하는 방법에 대해 이야기한다.
                    </p>
                    <p>
                        한 챕터를 잘 보여주는 좋은 TED강연이 있어 스튜어트 러셀의 강연으로 대신한다.
                    </p>
                </Text>
            </Flex>
        </Row>,
        date: "April 14, 19",
        chap: "Chap 6. The not-so-great AI debate, Chap 7. AI : a different approach",
    },
    {
        title: "인공지능의 신뢰성",
        // 0 - 43
        description: 
        <Col>
            <Text className="text">
            <Row>
                <Image width="20%" src="https://miro.medium.com/max/1320/0*DQ7P1UV1eB_wF6Up" />
                <Col style={{marginLeft: 15}}>
                    <p>
                        인공지능의 신뢰성, 즉 AI Reliability는 인공지능이 내포한 위험과 기술적 한계를 해결하고, 활용･확산과정에서의 위험･부작용을 방지하기 위한 가치 기준이라고 할 수 있다. 인공지능 윤리 실천과 이용자 인공지능 수용성 향상을 위한 핵심가치이다.
                    </p>
                    <p><strong>주요 요소로는 다음과 같다.</strong></p>
                    <ul>
                        <li>안전(Safety)</li>
                        <li>설명가능(Explainability)</li>
                        <li>투명(Transparency)</li>
                        <li>견고(Robustness)</li>
                        <li>공정(Fairness)</li>
                    </ul>
                </Col>
            </Row>
            <p><strong>국내외 주요 AI 윤리원칙(25개) 주요내용은 아래와 같다.</strong></p>
            <table style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>
                <tbody style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}} colspan="2"><b>제목</b>
                </th>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>주체</b>
                </th>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>수립목적</b>
                </th>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>주요 원칙</b>
                </th>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>주요 특징</b>
                </th></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>1
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Preparing for the Future of Artificial Intelligence (’16)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>President National Science and Technology Council Committee on Technology
                <p>(정부기관)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>美 정부의 입장</b>에서 AI 기술과 관련하여 나아갈 방향 제시한 <b>정부 보고서</b>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>공공선, 공정성, 안전, 투명성, 이해가능성, <b>선을 위한 AI(AI for good), 인간 가치(Human values)</b>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>AI를 주요 성장동력으로 보고 <b>美 정부의 역할 강조</b>
                <p><b>윤리원칙 제시보다는</b> 국제인도법에 근거한 AI 무기체계 개발 등 <b>다양한 AI 관련 이슈를 제시하는데 초점</b>
                </p>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>2
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Tenets (’16)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Partnership on AI
                <p>(민간 연구소)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>학계, 재계, 정책입안자 등 <b>다양한 주체들의 협력 도모</b>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>AI 혜택 최대화,</b>
                <p><b>다양한 주체들 간 협력,</b> 사생활보호, 견고함, 해악금지, 설명가능성
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>학계, 기업, 정책입안자 등 <b>다양한 주체들 간 협력을 강조하고</b>, 이를 통해 <b>대중교육 등</b> 추진할 것을 제안
                <p>기술 혜택 최대화의 전제로 사생활 보호, 연구공동체의 책임, 견고성, 해악금지 등 제시
                </p>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>3
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>AI Policy Principles (’17)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Information Technology Industry Council
                <p>(민간 협회)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>개발자에 대한 정부 차원의 지원</b> 및 공적 영역과 사적 영역의 협업 강조
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>안전과 제어가능성, 해석가능성, 인간 존엄성, <b>데이터의 대표성, 유연한 규제접근,</b> 기회의 평등
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>개발자의 입장을 강조, <b>정부의 규제나 개발자에 대한 정보공개 요구에 부정적</b>
                <p><b>다만 개발자에게도</b> 안전한 설계, 데이터 대표성 등 <b>높은 수준의 책임성 요구</b>
                </p>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>4
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>DeepMind Ethics &amp; Society Principles (’17)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>DeepMind
                <p>(기업)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>사내에서 AI 연구 수행시 윤리적 고려사항</b> 제시
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>사생활 침해 금지, 평등, 도덕성, 포용성, 안전과 책무성, <b>거버넌스·규제</b>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>연구자에게 필요한 윤리원칙과 체크</b>리스트를 제시하면서도 안전과 책무성을 <b>보장하는 거버넌스·규제 필요성 제기</b>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>5
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Asilomar AI Principles (’17)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Future of Life Institute
                <p>(민간 연구소)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>미국 보스톤의 비영리 연구단체인 삶의 미래 연구소 (Future of Life Institute) 주관으로 작성한 윤리원칙
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>인권보장, 개인정보보호, 해악금지, 공공성, 데이터 관리, 책임성, 통제성, 투명성, <b>무기경쟁 회피</b>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>스티븐 호킹·일론 머스크 등 <b>다수의 AI학자, 미래학자 및 산학연 관계자들이 서명</b>
                <p>AI 기술 연구자, 정책 입안자, 관련 산업 종사자에게 필요한 <b>윤리원칙 제시</b>
                </p>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>6
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>AI at Google: Our Principles (’18)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Google
                <p>(기업)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>구글 AI 개발자</b>에게 필요한 윤리원칙 제시
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>사회적 혜택 증진, <b>불공정한 편견 지양</b>, <b>설명가능</b>, <b>사생활침해 방지</b>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>개발을 제한해야되는 AI 어플리케이션</b>으로 해를 끼치는 기술, 인명을 해하는 무기관련 기술, 국제 규약 위반 감시기술 등 <b>제시</b>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>7
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Microsoft AI principles (’18)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Microsoft’s AETHER(AI and Ethics in Engineering and Research)
                <p>(기업)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>MS AI 개발자</b>에게 필요한 윤리원칙 제시
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>공정성, 신뢰성 및 안전, 사생활 및 보안, 포용성, 투명성, 책무성
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>MS 사내 윤리강령 성격</b>이 강하며, 책임질 수 있는 AI와 이를 위한 교육 강조
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>8
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>OpenAI Charter (’18)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>OpenAI
                <p>(민간 연구소)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>AI 기술 연구자</b>에게 필요한 윤리적 태도와 원칙 제시
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>공공선, 해악금지,
                <p>안전 담보, AI개발 선두주자, 타 연구단체 협력,
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>연구자의 자유로운 연구 증진에 초점,</b> 고도로 자율적인 AGI(artificial general intelligence) 상정
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>9
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Principles for Trust and Transparency (’18)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>IBM
                <p>(기업)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>IBM 직원들을 대상</b>으로 AI 연구를 위해 제시된 윤리원칙
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>인간 지능 증강(augment)</b>, 데이터 소유권, 국경간 데이터 이동, 투명성
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>AI는 인간을 대체하는 것이 아니라 증강(augment)하기 위한 것임을 명시</b>
                <p>AI 사용 여부·시기, 학습 데이터 출처 고지 등 규정
                </p>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>10
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>The Montreal Declaration for a Responsible Development of AI (’18)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>University of Montreal
                <p>(민간 대학)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>몬트리올 대학에서 개발된 사회적으로 책임 있는 AI 연구를 위한 윤리원칙
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>복지(well-being), 자율성 존중, 사생활 보호와 친밀성, 연대성, 민주적 참여, 공평, 다양성 포용, 사려
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>친밀성(intimacy), 사려(prudence), 지속가능한 발전 등 다른 가이드라인에 잘 등장하지 않는 원칙 제시</b>
                <p>윤리원칙 제시와 함께 서명으로 선언에 동참하도록 장려
                </p>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>11
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>지능정보사회 윤리가이드라인 (’18)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>정보문화포럼
                <p>(정부기관)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>인간 중심의 지능정보사회 구현
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>이용자 주도성</b>, <b>이용자/시민참여</b>, 공익, 공정성, 위험예방, 프라이버시 보호
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>지능정보기술 관련 개발자 및 공급자의 윤리의식 고취 및 이용자의 오남용 방지 지침
                <p><b>주체별(개발자, 공급자, 이용자) 세부지침 마련</b>
                </p>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>12
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>AI in the UK: Ready, Willing and Able? (’18)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>영국 정부
                <p>(정부기관)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>영국 정부 차원에서 정책적으로 접근</b>할 수 있는 제언 제시
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>데이터 접근과 제어, 이해가능한 AI, <b>디지털 이해력 증진</b>, <b>공중보건 관리</b>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>영국이라는 특정 국가 입장</b>에서 공중보건 데이터 관리, AI 디지털 이해력 제고 등 <b>구체적으로 취할 수 있는 AI 관련 정책을 제시</b>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>13
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>카카오 알고리즘 윤리헌장 (’18)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>카카오
                <p>(기업)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>카카오 내 AI 관련 연구</b>시 지향되어야 할 윤리원칙 제언
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>사회윤리 준수, 차별 경계, 학습데이터 운영, 알고리즘 독립성 및 설명,
                <p>기술 포용성, <b>아동·청소년 보호</b>
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>국내기업 최초 AI 윤리헌장</b>으로, 알고리즘과 데이터에 대한 관리, <b>아동과 청소년에 대한 보호 필요성  등 강조</b>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>14
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>The Future Computed: AI and its role in Society (’18)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Microsoft’s AETHER
                <p>(기업)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>AI가 가져올 미래의 변화에 대응하기 위해 MS의 Aether 연구소에서 책자 제작
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>AI에 의한 진보</b>, 공정성, 신뢰성 및 안전, 사생활·보안, 포용성, 투명성
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>AI가 경제·사회적 진보를 이끌고 지역적·전지구적 문제를 해결할 것이라는 관점</b>
                <p>AI가 직업과 직장에 미치는 영향에 공공부문과 민간부문이 협력해 대응할 필요성 제시
                </p>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>15
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Discriminating Systems - Gender, Race, and Power in AI (’19)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>AI Now
                <p>(민간 연구소)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>작업환경에서 다양성을 확보하기 위해 고려할 사항 제시
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>다양성, 해악금지, 개방성, 투명성
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>급여지급 기준의 인종별, 성별 공개, 직원 채용시 투명성 준수 등 제시
                <p>특히 <b>AI 시스템 사용시 투명성·편견·해악에 대한 철저한 점검·감시·추적·공개를 강조</b>
                </p>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>16
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Ethically Aligned Design(Ver. 2) (’19)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>The IEEE Global Initiative on Ethics of Autonomous and Intelligent System
                <p>(민간 학회)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>IEEE에서 Ethics in Action 캠페인과 함께 아울러 공개된 보고서
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>인권, 복지우선, 책무성, 투명성, 오용의 인식
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>각 원칙별로 이론적 배경, 참고자료를 제시하고 윤리원칙뿐만 아니라 관련 분야들에 대한 자료 수록
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>17
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>이용자 중심의 지능정보사회를 위한 원칙 (’19)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>방통위-KISDI
                <p>(정부기관)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>안전한 지능정보서비스 환경조성 및 이용자의 권리와 자유에 근거한 윤리원칙 제시
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>사람중심 서비스, 투명성과 설명가능성, 책임성, 안전성, 차별금지, 참여, <b>프라이버시와 데이터거버넌스</b>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>안전한 지능정보서비스 환경조성 및 이용자 보호를 위해 모든 주체 사이의 협력 강조
                <p>기업과 연구자들의 의견을 폭넓게 수렴하여 작성
                </p><p>이용자 보호의 관점 강조
                </p>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>18
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>로봇 윤리 기본 원칙(수정) (’19)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>산업통상자원부
                <p>(정부기관)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>2007년에 만들어진 로봇윤리헌장을</b>
                <p><b>수정 보완</b>
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>인간의 존엄성 보호, 공공선, 행복추구, 투명성, 제어가능성, 책무성, 안전성, 정보보호
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>로봇산업계에 종사하는 연구원, 개발자, 및 사용자</b>가 로봇과 AI을 설계·제작·공급·사용·관리하는 데 기준으로 삼는 가이드라인 제시
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>19
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>인간중심의 AI 사회 원칙 (’19)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>일본 총무성
                <p>(정부기관)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>25명의 산학연 전문가로 구성된 ‘인간 중심의 AI 사회 원칙 위원회’</b>를 통해 제안
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>인간중심, 교육교양, 개인정보 보호, 보안, 공정경쟁, 공정성, 책임성, 투명성, 혁신
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>저출산, 고령화, 지방쇠퇴, 재해 재난 등 일본이 처한 어려움을 AI가 해결</b>할 수 있을 것으로 상정
                <p>AI를 공공재로 활용하여 사회의 근본적인 변화와 혁신을 달성하여 지속 가능한 발전 추구
                </p>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>20
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Ethics Guidelines for Trustworthy AI (’19)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>EU
                <p>(국가 정부기관)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>EU 산하의 50여명으로 구성된 AI 전문가 그룹</b> 주도
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>인간 권리·자율성 보장</b>, <b>기술적 견실성</b>, <b>사생활</b>, <b>데이터 관리</b>, 투명성, 다양성, 차별금지, 복지, 책무성
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>범국가 차원의 협업을 통해 신뢰할 수 있는 AI를 위한 윤리원칙 정립</b>에 초점을 맞춤
                <p>각 원칙의 평가 리스트를 구체적으로 제시
                </p>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>21
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Recommendation of the Council on AI (’19)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>OECD
                <p>(범국가 정부기관)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>OECD 디지털 경제 정책 위원회</b> 주관하에 제작
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>포용적 성장</b>, <b>지속가능 발전</b>, <b>인간중심 가치</b>, 공정성, 투명성, 설명가능성, 견고성, 보안 및 안전, 책무성
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>윤리원칙 뿐 아니라 정책 입안자들에 대한 제언 제시, <b>국가별 정책 수립과 국제적 협력 도모</b>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>22
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>The global landscape of AI ethics guidelines (’19)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Jobin. A., Ienca, M. &amp; Vayena, E.
                <p>(개인)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>전 세계의 주요 84개의 AI 윤리 가이드라인을 분석
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>투명성, 정의, 해악금지, 책임, 사생활보호, 혜택 추구, 자유, 신뢰, 지속가능성, 연대성
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>주요 윤리원칙을 빈도수별로 분석하고 주로 선진국을 중심으로 발표되고 있음을 밝힘</b>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>23
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Understanding artificial intelligence ethics and safety (’19)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>The Alan Turing Institute
                <p>(국영 연구소)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>영국의 국영 연구소</b>인 Alan Turing 연구소에서 제작
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>존중, 연결, 보호, 돌봄, 공정성, 책임성, 지속가능성, 투명성
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>AI 기술이 <b>데이터를 처리할 때 발생할 수 있는 위험이나 문제점을 예방하는 데 필요한 윤리원칙에 초점</b>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>24
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Principles Artificial Intelligence: A Map of Ethical and Rights-Based Approaches (’20)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Berkman Klein Center For Internet &amp; Society
                <p>(민간 연구소)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>36개의 윤리 가이드라인에 등장한 윤리원칙들을 주제별로 분석
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>사생활보호, 책무성, 안전과 보안, 투명성과 설명가능성, 공정성과 차별금지, 인간의 기술통제, 전문적 책임
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>정부, 정부 기관, 사적 기관 등 다양한 주체들이 제시한 윤리원칙들을 8개의 주제로 분류하고 분석</b>
                </td></tr>
                <tr>
                <th style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>25
                </th>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>Rome Call for AI Ethics (’20)
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>로마 교황청
                <p>(민간기관)
                </p>
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>로마 교황청</b>에서 인간의 혁신적인 미래를 위한 AI 윤리원칙 제정
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}>투명성, 포용성, 책임성, 불편부당성, 신뢰성, 보안과 사생활 보호
                </td>
                <td style={{border: '1px solid white', textAlign: 'center', verticalAlign: 'middle'}}><b>종교 기관인 가톨릭교회에서 제정</b>한 윤리 원칙으로, <b>인간 가족(human family)에 대한 봉사, 젊은 세대에 대한 준비, 자연의 회복 필요성</b> 등 제시
                </td></tr></tbody></table>
            </Text>
        </Col>,
        date: "April 26, 28",
        part: "한상기 - 신뢰할 수 있는 인공지능",
        chap: "1장. 인공지능의 신뢰성"
    },
    {
        title: "인공지능의 공정성",
        // 60 - 87
        description: <Col>
            <Text className="text">
                <p>
                인공지능 편향은 사회적 영향과 거버넌스의 문제일 뿐만 아니라 인공지능 시스템의 강건성
                문제이기도 하다. 부호처리 패러다임의 인공지능에서는 제기되지 않던 인공지능 편향 문제는
                컴퓨터가 인공신경망 기반의 자율지능시스템 단계가 되면서 시스템 구축 절차 각각에서 개입
                된다. 공정성의 다양한 유형들은 동시에 만족되기 어
                렵고 인공지능의 적용 분야 및 맥락에 따라 상이한 기준과 요소의 결합이 필요하다. 학습 데
                이터, 분류자, 예측 내용의 편향을 완화하는 방법 또한 편향을 완전히 차단하는 것은 아니며
                편향완화와 정확도 간의 조화를 모색해야 한다. 인공지능 감사를 통해 알고리즘에 무제한으
                로 접근하여 편향을 식별해낸다 하더라도 해당 알고리즘의 편향 여부를 단정하기는 어렵다. 편향완화 기술은 단순히 편향을 제거하는 단계를 넘어서서 편향완화와 시스템의 강건성을 동
                시에 확보하는 과제, 그리고 다양한 공정성 유형들을 조정하는 과제를 해결하는 단계로 나아
                가고 있다. 결론적으로, 이러한 특성들은 인공지능 편향을 인지하고 해결책을 모색하는 과제
                가 개념적 차원의 사안 인지를 넘어서 시스템 이해에 기반한 편향 인식 및 조정 차원에서 모
                색되어야 함을 암시한다.
            </p>
            <p>
                인공지능 편향 이슈가 기존의 부호처리 방식의 컴퓨터 발전 단계가 아니라 딥러닝이라
                는 정보 처리방식이 만연하게 된 현재에 등장
                한 배경은 편향이 인공지능 시스템의 설계 및
                구성에 내재해 있음을 암시한다. 그런 의미에
                서, 인공지능 편향을 인지하고 완화하고자 할
                때 인공지능에서 주요한 비중을 차지하는 데
                이터 전처리와 기계학습을 이해하고 그 과정
                에서 편향이 개입되는 지점들을 인지하는 것
                이 중요하다. 인공지능이 사회의 전 영역에서
                활용되고 또한 역으로 우리의 판단과 행동에
                영향을 주고 있음을 고려하면, 인공지능 편향
                에 대한 감수성이 필요하다.
            </p>
            <p>
                인공지능 편향은 인간이 만들어 기계 안에
                내재하게 되며 어느 정도 조정가능한 만큼 이
                에 대한 교육은 기존의 개념중심의 학습이 아
                닌 경험 및 체험 (hands-on) 방식의 학습이 필
                수적으로 포함되어야 인공지능 개발자뿐만 아
                니라 사용자로서도 편향에 의한 부작용의 피
                해를 방지할 수 있다. 인공지능 편향의 시스
                템적 이해를 위해서는 상술한 인공지능 구성
                절차의 일부라도 체험해보는 것이 도움이 된
                다. 현재 어린이도 경험가능한 기계학습 플랫
                폼이 다양하게 존재하며 수준을 다양하게 조
                정할 수 있다. 이를 통해 데이터의 종류를 달
                리해보고, 변수를 변화시켜 봄으로써 비교적
                간단하게 기계학습의 원리와 편향의 발생하는
                지점을 직접 이해할 수 있다.
            </p><p>
                이러한 학습방식은 비단 인공지능 산업 관
                련 개발자뿐만 아니라 인공지능 기반의 제품
                을 사용하는 일반인들에게도 유용하다. 일반
                인들은 인공지능의 설계, 제작에는 참여하지
                않지만 인공지능 기반의 시스템이 내리는 의
                사결정에 영향을 크게 받는다. 따라서 인공지
                능의 구성과정과 편향이 개입되는 지점, 조정
                가능하다는 사실과 이것이 사회의 거버넌스와
                연관되어 있다는 점을 인지하면 세계적으로
                적용되고 있는 인공지능윤리 관련 권리를 정확히 행사하여 시민권을 지킬 수 있다. 또 사회로 나가기 전의 젊은이들은 인공지능을 활용하는 모든 영역에서 보다 나은 활용을 할
                수 있다.
            </p><p>
                인공지능 편향에 대한 상술한 적절한 인지를 위해서는 인공지능 공정성이나 인공지능
                편향완화 기술에 대한 막연한 기대보다는 인공지능 시스템 구성 절차와 거버넌스 과정에서 공정성의 요소를 고려하는 것이 필요하다. 본문에서 소개된 공정성의 정의들이나 편향완화 알고리즘들은 다양하지만 상충하는 사례들도 있으며, 어느 하나가 대표적이거나 완벽한
                해결책이 될 수 없다.
                그 이유는 다양한 사례들이 한 유형의 공정성이나 편향완화 알고리즘만으로 해결되기 어려운 복잡한 맥락을 가지고 있다는 점에 그치지 않는다. 여러 유형의 공정성을 아우르는
                알고리즘이 개발된다고 할지라도 많은 경우에
                공정성 즉 편향완화가 이루어질수록 모델의
                정확도가 낮아지기 때문이다. 상기에 소개한 몇몇 사례들처럼 편향을
                완화하면서도 시스템의 강건성 또한 확보하는
                새로운 기술들이 연구되고 있는 반면, 양자를
                모두 만족시키기란 쉽지 않다. 그렇다면 자연스런 대안은 편향과 정확도 간에 ‘균형’을 찾는 것이다. 이 균형을 설정하기 위해서는 인공지능 기술의 발전만으로는 불가능하며, 사회의 이해관계자들 간의 논의와 합의를 필요로 한다. 기술과 사회적 거버넌스, 그리고 이
                과정에서 인간의 심리적 편향의 검토와 완화의 과정은 새로운 공정성 개념들과 편향완화
                기술들을 만들어 내게 될 것이다.
                </p>
            </Text>
        </Col>,
        date: "May 3, 10",
        part: "한상기 - 신뢰할 수 있는 인공지능",
        chap: "2장. 인공지능의 공정성"
    },
    // {
    //     title: "발표",
    //     description: "",
    //     date: "May 12",
    // },
    // {
    //     title: "발표",
    //     description: "",
    //     date: "May 17",
    // },
    {
        title: "인공지능의 윤리성",
        // 109 - 156
        description: 
        <Col>
            <Text className='text'>
            <p>인공지능은 거의 모든 산업에 적용되어, 복잡한 문제 해결, 생산성‧효율성 증가, 비용 절감 등 경제적 가치를 창출하지만, 사회‧윤리적으로 예상하지 못한 부작용도 초래하고 있다.
            </p>
            <ul>
                <li>- 우리나라의 AI 챗봇 ‘이루다’가 성소수자나 장애인에 대한 혐오 발언으로 서비스를 종료하거나 딥페이크, 자율주행자동차 사고, 자율살상무기 등을 통해 AI의 위험성이 경고되고 있다.</li>

                <li>- 2015년 이후 AI 컨퍼런스에서 ‘윤리(ethics)’ 키워드가 포함된 제목의 논문이 증가하는 등 세계 각국은 AI 윤리의 중요성을 인식하고 있다.</li>

                <li>- 이에 따라, 다가오는 AI 시대에 대비하여 신뢰할 수 있는 AI 실현을 위해 발생 가능한 사회‧윤리적 쟁점에 대해 지속적인 논의가 필요하다.</li>
            </ul>

            <p>
            세계 각국은 인공지능의 활용‧확산 과정에서 위험이나 부작용 등을 방지하기 위한 방안을 마련하고 있다.</p>
            <ul>
                <li>- 2019년 5월, 42개 국가는 AI 시스템을 공정하며 신뢰할 수 있는 방식으로 설계하는데 동의하는 AI에 관한 OECD 원칙에 서명했다.</li>

                <li>- EU에서는 2019년 4월 ‘신뢰할 수 있는 AI를 위한 윤리 지침’을 제시하였고, 2021년 4월 세계 최초로 ‘인공지능 법안(Artificial Intelligence Act)’을 발표했다.</li>

                <li>- 미국에서도 2019년부터 AI를 규제하기 위한 법안을 도입하고 있으며, 구글, 마이크로소프트 등의 주요 기업을 중심으로 윤리적인 AI 실현을 위한 자율적인 AI 개발 원칙을 마련했다.</li>
            </ul>

<p>
            하지만 아직까지 국가 차원의 인공지능 윤리는 큰 틀에서의 기준만 마련되어, 보다 세부적인 논의를 바탕으로 사회적 합의가 이루어져야 한다.
            </p>
            <ul>
                <li>- 고위험 AI에 대한 명확한 기준을 설정하여, 사전에 발생 가능한 문제에 대해 지속적인 논의를 통해 예방할 수 있어야 한다.</li>
                <li>- AI 시대의 다양한 상황에서 어떻게 AI가 사람 중심으로 의사결정을 할 것인지 등에 대해 구체적인 논의가 필요하다.</li>
                <li>- AI 윤리는 AI 자체라기보다 AI를 개발하고 운영하는 규범으로의 윤리라는 점을 인식해야 한다.</li>
            </ul>

            <p>
            인공지능 활용 활성화를 위해 기술적 측면과 사회‧윤리적 측면을 모두 반영한 신뢰할 수 있는 인공지능 실현 방향을 마련해야 한다.
            </p>
            <ul>
                <li>- 현재 AI와 관련된 편향성, 불투명성, 안전성 등 AI의 불완전성을 해결하기 위해 충분한 학습 데이터 구축, 설명 가능한 AI 구현, 표준화된 AI 가이드라인 수립 등 과학기술적 측면의 노력이 요구된다.</li>

                <li>- 예방적인 차원에서 AI 윤리에 대해 논의하고 구체적인 기준 마련, 사회적 합의 등을 통해 AI의 사회‧윤리적 이슈에 잘 대처해야 한다.</li>

                <li>- AI 윤리와 관련된 이슈를 총괄하고 조정하는 거버넌스를 구축할 필요가 있으며, 모든 사람들과의 공유를 통해 신뢰할 수 있는 AI를 위한 인식을 확산해나가야 할 것이다.</li>
            </ul>        
        </Text>
        </Col>,
        date: "May 31, June 2",
        part: "한상기 - 신뢰할 수 있는 인공지능",
        chap: "인공지능의 윤리성"
    },
    {
        title: "Su Hyung Choi's Philoshopy",
        description: 
        <Col>
            <Text className="text">
                <p>
                    인공지능 윤리 수업을 들으며 가장 인상적인 점은 한 학기동안 정말 많은 생각을 스스로 할 수 있었다는 것이다. 의미있는 주제들을 통해서 인공지능, 또 인공지능에 결부된 여러가지 사회문제와 야기될 윤리적 문제, 그리고 무엇보다도 인공지능을 넘어서 인간성과 그 존엄성, 근본적인 가치와 미래의 향방을 고민할 수 있었기 때문이다. 나의 과제의 목차는 아래와 같다.
                </p>
                <ul>
                    <li>1. 3월 8일 Assignment: Q. 인간 면접관과 인공지능 면접관은 모두 비슷한 문제를 내포하고 있다고 볼 수 있다. 대표적인 것이 편향성의 문제인데, 왜 발생을 하고 어떻게 해결할 수 있을 것인지에 대해 인간면접관, 인공지능 면접관을 비교하여 설명하시오.</li>
                    <li>2. 3월 15일 Assignment: Q. 포털 사이트의 뉴스 추천, SNS의 포스트 글 또는 친구 추천 등의 알고리즘의 사례 및 문제점을 조사하여 정리하시오.</li>
                    <li>3. 3월 31일 Assignment: Q. 오늘 수업시간에 이야기 했던 것 처럼, ‘언어와 상식’의 부분은 단순한 언어인식의 수준과는 다른 차원이며, 이 부분을 어떻게 만들 수 있을 것인가는 인공지능에서도 가장 큰 어려운 난제 중의 하나이다. 이와 관련된 실험이 하나 있었는데, 바로 일본에서 진행했던 도쿄대학 합격 로보트 만들기 프로젝트였다. 관련된 자료를 찾아보고, 어떤 과목에서 어려움을 겪었는지 그 이유는 무엇인지, 그것의 해결이 가능할 것인지, 그 것의 해결을 위해서는 무엇이 준비되어야 하는지, 관련 내용을 정리하고 당신의 생각과 의견을 적으시오.</li>
                </ul>
            </Text>
        </Col>,
    },
    {
        title: "March 8 Assignment",
        expansion: true,
        description: 
        <Col margin="margin-top: 20px;">
            <Text style={{margin: '0 20%'}} color={unlimitColor} lh="30" size="16.5">Q. 인간 면접관과 인공지능 면접관은 모두 비슷한 문제를 내포하고 있다고 볼 수 있다. 대표적인 것이 편향성의 문제인데, 왜 발생을 하고 어떻게 해결할 수 있을 것인지에 대해 인간면접관, 인공지능 면접관을 비교하여 설명하시오.</Text>
            
            <Text margin="margin: 20px 0;" lh="30">
                <p className="marginP">‘편향성’이라는 문제에서 인간과 인공지능은 둘다 자유로울 수 없다. 이는 결국엔 특정한 논리에 의해서 작동하는 인간의 사고와 인공지능의 지능이 결코 어떠한 부분에서도 객관적일 수 없다는 면을 보여주는데, 이는 그렇다면 근본적으로 ‘결정’이라는 측면에서 과연 주관적이지 않은 것이 있냐는 철학적 문제로 귀결될 수 있다. </p>
                <p className="marginP">인간면접관과 인공지능면접관의 판단은 결국 특정 ‘주체’에 의해 결정되거나, 지도된 ‘GUIDELINE’으로써 (인간의 경우엔 지침이나 규정 따위, 인공지능의 경우엔 해당 지능을 설계한 인간의 전문가적 관점 내지는 명시적 알고리즘) 결정을 내리던, 아니면 둘다 각각 ‘객체’의 학습에 기반하여 (인간의 경우, 상호간의 살아오거나, 학습으로부터 경험된 일련의 지식, 인공지능의 경우에는 지도, 또는 비지도 학습을 통해 데이터로부터의 일련의 ‘경험’) 결정을 내리던 결국엔 그 근반이되는 모든 ‘지식’이라는 정보는 ‘벡터성’, 즉 방향성을 지니고 있다. </p>
                <p className="marginP">하지만, ‘보편적’이라는 관점에서 인간은 사회적인 상식이 존재하고, 내지는 문화적이고 범용적 인간으로써의 규범이 존재한다. 인공지능은 인간이 디자인하였고, 인간을 위한다는 가정을 전제로한다면, 인공지능의 규범과 ‘일반적인’ 도덕과 그 객관성은 인간의 객관성을 따르게 된다는 것이다.</p>
                <p className="marginP">결국은 인간이 어떻게 사고하고, 판단하는지에 따라서 현재 수준의 인공지능의 도덕성과 객관성은 결정될 수 밖에 없다. 그것이 ‘비지도’이던, ‘지도’이던 결국엔 인공지능이 설계되는 과정 - 1. 특정한 인간의 지식이 EMBEDDING (전문가 시스템),  2. 데이터를 통한 학습 – 에는 근본적 바탕이 되는 정보장 (INFORMATION FIELD)이 존재하며, 이는 인간이 지금껏 쌓아왔던 ‘업’의 결과로 판단되고 경험되어진다고 생각해볼 수 있다.</p>
                <p className="marginP">정리하자면, ‘인간’이라는 주체는, 행동하고 사고하는 주체이며, 유전적으로 내제된 판단력 – 이 또한 장시간의 걸쳐 진화로써 학습된 결과라라고 볼 수 있다. – 과 인생을 걸쳐 경험을 통해 학습한 판단력을 지니고 있는 존재이며, ‘인공지능’이라는 존재는 결국엔 이가 스스로 판단력을 갖춘 ‘주체’로써의 존재이던, 혹은 정해진 법칙에 의해 판단하던 ‘객체’로써의 존재이던, 인공지능의 판단력은 결국은 ‘인간’의 규범을 따를 수 밖에 없다는 것이다. – 특히 최근의 데이터로써 학습되는 기계학습 중점의 인공지능은 더더욱 그렇다.-. 때문에 인간의 편향성이 그대로 인공지능에게 답습되고, 이는 때로는 의도하지 않았지만 인간이 지금껏 쌓아온 정보장에서 편향되어 있는 정보에 기반하여 학습되어 인간보다 때때로 더더욱 편향적인 모습을 보이곤 한다.</p>
                <p className="marginP">결국 다시 문제는 인간으로써 귀결된다. 우리는 인공지능을 통해 인간으로써의 우리의 존재를 다시 탐구해보아야 한다. 적어도 인공지능이라는 존재를 우리의 거울로 만들 것인지, 아니면 더 나은 존재 내지는 인간을 보조하며, 더 나은 결정을 내리게 하는 존재로 만들 수 있는지 조차 고민해 볼 필요가 있다.</p>
                <p className="marginP">사실 기계적인 대답으로써 편향성의 극복은 인간면접관에게는 사회 보편적인 규범 교육 내지는 그러한 도덕성을 바탕으로한 원리, 원칙에 의한 판단, 그리고 인공지능 면접관에게는 올바른 데이터와 편향성을 가지지 않은 지도된 데이터 필드에서 OVERFITTING과 UNDERFITTING을 방지하고, 좀 더 GENERAL한 성능을 가진 인공지능을 설계하는 것이 되겠다. 하지만 나는 이것 이전에 근본적인 인간으로써의 존재와 이를 넘어서게 될 가능성이 존재하는 인공지능을 설계하는 핵심적 철학에 대해서 연구가 선행됨이 당연하다고 생각한다. 인간이 존엄하고, 고귀한 이유가 바로 그것이기 때문이지 않을까.</p>
            </Text>
        </Col>,
    },
    {
        title: "March 15 Assignment",
        expansion: true,
        description: 
        <Col margin="margin-top: 20px;">
            <Text style={{margin: '0 20%'}} color={unlimitColor} lh="30" size="16.5">Q. 포털 사이트의 뉴스 추천, SNS의 포스트 글 또는 친구 추천 등의 알고리즘의 사례 및 문제점을 조사하여 정리하시오.</Text>
            
            <Text margin="margin: 20px 0;" lh="30">
                <p className="marginP">
                많은 기업들이 사용자를 그들의 platform에 양식하기 위해서 많은 알고리즘을 도입하고 있는데, 그 대표적인 알고리즘 중 상당수가 machine learning을 이용한 알고리즘이다. 해당 machine learning기법은 주로 recommender system을 위해서 급격하게 발달하였으며, 전통적인 machine learning algorithm인 clustering과 classification 등을 이용하여 많은 결과를 낳았다. 최근에는 Graph Neural Network, 즉 여러 데이터들을 하나의 거대한 data points, 즉 데이터들을 특정한 node로써 취급하여 상호간의 연결된 graph에서 연결성을 추출하는 algorithm등을 선보이고 있다. 
                </p>
                <p className="marginP">
                Social Media는 분명 우리 삶에 지대한 영향을 미치고 있다. 나의 친구, 그 친구의 친구를 넘어서 전세계를 3다리만 건너면 알 수 있게 만들어버린 이 network에서 platform기업들이 가지는 영향력은 그야말로 국가를 초월하며, 그 규제 또한 애매모하고 쉽지 않다. 이러한 점들을 인식하였는지 2016년부터 미국 트럼프 대통령 당선을 계기로 흔히 말하는 ‘fake news’의 파급력에 대해서 전세계가 주목하였으며, 트럼프 대통령의 임기말인 2020년에는 미 국회난동사건으로 자국 대통령의 계정을 중지시켜버리는, 그야말로 절대권력을 지니고 있기도 하다.
                </p>
                <p className="marginP">
                하지만 사람들은 이러한 Social Media의 영향력을 크게 간과하고 있는데, 무의식적으로 5분마다 핸드폰을 켠다는 통계가 존재하는 현실 속에서 우리는 그대로 무방비하게 platform의 사고방식에 노출되고 있다.
                </p>
                <p className="marginP">
                최근에 이러한 Recommeder system에 대한 윤리적 연구도 이루어지고 있는 것으로 보이는데, Oxford에서 Ethical A.I, 최근에는 recommeder system에 대한 윤리적 문제를 연구하고 있는 Silvia Milano는 우리가 사용하는 Recommeder System의 ‘Stakeholders’, 즉 이 추천시스템으로부터 가장 이익을 보는 집단이 누군가를 알아야한다고 강조하고 있다. 이 부분이 굉장히 흥미로운데, 그녀가 출연한 youtube에서 introduction에서 아래와 같이 말하고 있다.
                </p>
                <p className="marginP" style={{fontFamily: "Cormorant garamond", fontStyle: "italic", textAlign: 'center', fontSize: 18}}>
                    “we'll be zooming into is this question of 
                    <span style={{color: 'red'}}>who the stakeholders are in modern recommender systems</span> so is it just about the end user is it just about the person who interacts with the twitter app or with netflix or whatever it may be or do we have to start zooming out a little a little bit more and thinking about companies thinking about systems thinking about even societies and civilizations”
                </p>
                <p className="marginP">
                    결국에 이러한 추천시스템이 platform사용자, 즉 end user를 위한 것인지, 최종적으로는 기업이나 특정 단체를 위한 것인지에 대해서 우리는 알 필요가 있다는 것이다.
                </p>
                <Col width="60%" margin="margin: 0 20%">
                    <Image style={{width: '100%'}} of="contain" src={as2_1} />
                    <Text margin="margin-top: 10px;" style={{fontFamily: "Cormorant garamond", textAlign: 'center', fontSize: 18}}>Reference from <Text style={{fontStyle: 'italic', fontSize: 18}}>Recommeder System and their Ethical System</Text></Text>
                </Col>
                <p className="marginP">
                    모든 정보는 ‘벡터성’을 지니고 있다. 그것이 news이던, 우리가 아무생각 없이posting하는 글이던, 은연중 자랑하기 위해 올리는 글이던, 모든 것은 특정한 목적을 지녔다는 의미다. user개인도 이러한데, 이 user들의 뭉친 집단적인 특성은 특정한 주제를 중심으로 더 편향되고 강력한 벡터성을 형성하기 쉽다. 흔히 ‘집단지성’으로 대표되는 다수의 의견과 사고의 힘은 platform에서 발현될때도 있지만, 이것은 user개개인의 합으로 이루어지기 보다 platform의 정해진 flow에 의해서 이루어질 가능성이 높은 것이다. 마치 하나의 ‘pipeline’처럼 말이다.
                </p>
                <p className="marginP">
                상기 언급한 논문에서는 Recommeder System의 Ethical Problem을 다음과 같이 지적하고 있는데, 크게는 아래와 같다.
                </p>
                <ul className="lister">
                    <li>
                        Inapproriate content. (부적적한 contents) - Only a handful of studies to date address explicitly the ethics of Recommeder system as a specific issue in itself- 해당 논문에서는 ‘특정한 이슈’뿐만 아니라 전체적인 부분에서 Recommeder System의 윤리적 문제가 드러나고 있는데에 반해 특정한 이슈에만 한정하여 연구가 진행되고 있다고 강조한다.
                    </li>
                    <li>
                        Privacy (개인정보)
                    </li>
                    <li>
                        Opacity (투명성)
                    </li>
                    <li>
                        Fairness (공정성)
                    </li>
                    <li>
                        Social Effects (사회적 영향)
                    </li>
                </ul>
                <Col width="60%" margin="margin: 0 20%">
                    <Image style={{width: '100%'}} of="contain" src={as2_2} />
                    <Text margin="margin-top: 10px;" style={{fontFamily: "Cormorant garamond", textAlign: 'center', fontSize: 18}}>Reference from <Text style={{fontStyle: 'italic', fontSize: 18}}>Recommeder System and their Ethical System</Text></Text>
                </Col>
                <p className="marginP">
                    끝으로, 많은 사회학적, 철학적, 사회과학적, 그리고 컴퓨터과학적 분야에서, 해당 문제들이 거의 다루어지지 않고 있다는 점을 지적하면서, 해당분야가 앞으로 더 심해질 Recommeder System의 문제에서 연구될 여지가 많이 남겨져있다는 것을 보인다. 
                </p>
                <Col width="60%" margin="margin: 0 20%">
                    <Image style={{width: '100%'}} of="contain" src={as2_3} />
                    <Text margin="margin-top: 10px;" style={{fontFamily: "Cormorant garamond", textAlign: 'center', fontSize: 18}}>Reference from <Text style={{fontStyle: 'italic', fontSize: 18}}>Recommeder System and their Ethical System</Text></Text>
                </Col>
                <p className="marginP">
                    나도 이에 전적으로 동의하며, 이 문제는 Computer Science와 Literatural 분야를 넘어서 전 인류를 관통하는 큰 문제가 될 수도 있다고 생각한다. A.I, Blockchain (탈중앙 시스템) 등 현재 전쟁을 겪고 있는 인류만 봐도 이러한 Ethical한 방향을 중심으로 기술이 발전하고, <u>무엇보다 이 기술을 선도하는 platform기업을 강력하게 제재, 내지는 무너뜨릴수 있는 Conensus에 대한 연구가 가장 중요한 시점이라고 생각한다.</u>
                </p>
                <ol className='lister'>
                    <Text>Reference</Text>
                    <li>Recommender System and their ethical challenges (Silvia Milano · Mariarosaria Taddeo · Luciano Floridi, Oxford University)</li>
                    <li>https://link.springer.com/article/10.1007/s00146-020-00950-y</li>
                    <li>https://towardsdatascience.com/ethical-problems-with-recommender-systeems-398198b5a4d2</li>
                    <li>https://youtu.be/Ng13EoLMCS8</li>
                </ol>
            </Text>
        </Col>,
    },
    {
        title: "March 31 Assignment",
        expansion: true,
        description: 
        <Col margin="margin-top: 20px;">
            <Text style={{margin: '0 20%'}} color={unlimitColor} lh="30" size="16.5">Q. 오늘 수업시간에 이야기 했던 것 처럼, ‘언어와 상식’의 부분은 단순한 언어인식의 수준과는 다른 차원이며, 이 부분을 어떻게 만들 수 있을 것인가는 인공지능에서도 가장 큰 어려운 난제 중의 하나이다. 이와 관련된 실험이 하나 있었는데, 바로 일본에서 진행했던 도쿄대학 합격 로보트 만들기 프로젝트였다. 관련된 자료를 찾아보고, 어떤 과목에서 어려움을 겪었는지 그 이유는 무엇인지, 그것의 해결이 가능할 것인지, 그 것의 해결을 위해서는 무엇이 준비되어야 하는지, 관련 내용을 정리하고 당신의 생각과 의견을 적으시오.</Text>
            
            <Text margin="margin: 20px 0;" lh="30">
                <p className="marginP">
                    ‘What is the Context?’ 최근에 내가 딥러닝 많은 논문들을 읽으면서 느낀 걸 한문장으로 요약하자면 다음과 같다. ‘Context’, 즉 언어에서는 문맥이고, 비전에서는 객체를 둘러싸고 있는 상호 의존적 Scene이며, Agent에게는 환경에서의 특정 action의 heuristic(단서)로써 작용한다. 이러한 Context는 현재 인공지능에게 가장 중요한 키워드 중 하나이며, 적어도 2022년 현재 자연어처리와 비전분야에서는 더더욱 각광받는 주제라고 할 수 있다.
                </p>
                <p className="marginP">
                    Ted영상에서 (4:25) “Unfortunately, none of the modern AIs, including Watson, Siri and Todai Robot, is able to read. But they are very good at searching and optimizing.“ 라는 말을 사회자가 하는데, 이는 즉, modern A.I는 안타깝게도 읽을 순 없지만, 대신에 검색과 최적화를 잘한다고 한다.  또한 이런 내용이 나온다. (5:42) “Our robot does not read, does not understand, but it is statistically correct in many cases.“ 
                </p>
                <p className="marginP">
                    결국엔 로봇은 읽지도 못하고, 이해도 못하는데, 통계적으로 정답을 알아맞춘다는 것이다. 그렇다면 혹자는 이렇게 생각이 들 수도 있다. 
                    <strong>‘이것을 A.I 라고 할 수 있을까? 그냥 특정한 상황에서 최적화된 Search를 제공하는 컴퓨터에 불과하지 않는가? 애초에 A.I의 명확한 정의는 무엇인가?’</strong>
                    이는 즉 modern A.I에게 어떠한 문맥과 상황을 파악할 수 있는 능력이 없다는 것인데, 그렇다면 우리는 생각해볼 필요가 있다. 
                    <strong>‘읽다의 정의는 무엇일까? 읽고 이해함의 정의의 기준은 무엇이고, 그 최소의 능력은 과연 무엇이란 말인가?’</strong>
                </p>
                <p className="marginP">
                    흥미롭게도 나는 최근 이러한 것들의 단서를 조금씩 찾아가기 시작하였으며, 놀라운 것은 이에 대한 단서는 지난 20년간 지속적으로 제기되어왔으나, 이를 명시적인 알고리즘으로 나타낸 것은 불과3~4년 사이라는 것이었다.
                </p>
                <p className="marginP"><strong>‘Connectivity’</strong></p>
                <p className="marginP">
                    가장 밑단에서부터 고민해본다면, 우리는 Deep Learning, 즉 심층신경망과 그것을 이루는 가장 작은 단위인 Perceptron이 인간의 뇌를 본따 만들었다는 것을 안다.
                </p>
                <p className="marginP">
                    ‘Connectivity’, 즉 연결성이란 작은 (기껏해봐야 특정 임계이상일 때 신호를 정제해주는) 단순한 작동을 하는 유닛들의 집합이 연결되고 모여서 특정한 군집을 형성하게 되고, 이러한 패턴들은 특정한 ‘연결성’을 다시 형성하게 된다. 즉, 작은 것들의 신호가 특정한 신호처리를 위해서 일련의 연결성을 띄고, 그 연결성에 따라서 우리는 거대하고 많은 복잡한 신호를 처리하는 것이다. 결국은 우리는 일련의 사고, 즉 먹고, 그림을 그리고, 글을 쓰고, 사고하는 모든 것들이 이 기본 단위의 집합에서 비롯되었으며, 이 단순한 개체의 특정한 연결성이 우리에게 일련의 행동과 사고를 비롯하게 한다는 것을 유추할 수 있다. 딥러닝은 이러한 유닛을 잘 쌓아 좋은 performace를 내는 인간이 만들어낸 도구중 하나이다. 우리는 이러한 심층신경망이 어떠한 의미에서 연결성을 통해 특정한 신호를 처리하는지 어느정도 이해할 수 있는 수준에 와있다.
                </p>
                <p className="marginP">
                    <strong>Context in Vision: Convolutional Neural Networks</strong>
                </p>
                <p className="marginP">
                    CNN에서의 필터는 조금 다르다고 생각될 수 있는데, 결국엔 각 레이어마다 필터의 Activation을 따라 특정 신호가 ‘강조’된다는 면에서 일반 MLP와 크게 다르지 않다. 다만 우리는 여기서 ‘시각적인 연결성’, 즉 시각적인 ‘문맥’을 더 파악할 수 있는데, 이것은 우리에게 특정한 객체 주위에 어떠한 문맥이 있을지 좋은 단서를 제공해준다. 다만 이는 receptive fields(필터)의 한계상 특정 영역에서의 단서, 즉 Locality를 통해서 우리에게 heuristic을 주는 것이다.
                </p>
                <p className="marginP">
                    <strong>Go Back to the points…</strong>
                </p>
                <p className="marginP">
                    결국엔 우리가 이해한다는 것은 여러 작은 단위 (그게 퍼셉트론이던, 소설 속 수많은 단어의 개별 집합이던 … )의 일련의 ‘연결성’이 가중됨에 따라서 특정한 이해를 하는 것이며, 이를 ‘문맥’이라고 할 수 있음을 유추할 수 있다. 이는 단순히 추상화된 언어의 영역에서 뿐만 아니라 비젼분야에서 먼저 발전되었는데, 이는 연결성이 곧 이해이며, 그 연결성의 강도 (MLP에서의 Weight, CNN 에서의 receptive fields) 들이 곧 ‘문맥’이 됨을 인지한다.
                </p>
                <p className="marginP">
                    <strong>Context in Natural Language</strong>
                </p>
                <Col width="60%" margin="margin: 0 20%">
                    <Image style={{width: '100%'}} of="contain" src={as3_1} />
                    <Text margin="margin-top: 10px;" style={{fontFamily: "Cormorant garamond", textAlign: 'center', fontSize: 18}}>Figure 1 ) Probabilistic Approach of NLP</Text>
                </Col>
                <p className="marginP">
                    초기에 자연어처리 모델에서의 문맥은 확률적인 모델에 근사하여 등장하였다. 흥미로운점은 자연어 처리의 특성상 초기부터 ‘문맥’의 개념을 도입해서 모델을 구성하였다는 것인데, 이점에서 기계가 완전히 이해를 못한다고 생각할 수는 없다. 적어도 잘 모아지고 잘 학습된 데이터셋에서는 좋은 성능과 좋은 결과를 보일 수 있다는 점이 그것이다. 하지만 이러한 ‘serial’(일련의) 모델 구성은 여러 한계가 있는데, 바로 병렬연산이 불가능하다는 것이고, 다른 하나는 context간의 gradient가 vanishing되기 쉽다는 것이다. 즉 이는 개념은 좋았지만, 완전한 의미에서의 context가 학습이 힘들었던 것이다.
                </p>
                <p className="marginP"><strong>Attention is All You Need (NIPS 2017)</strong></p>
                <Col width="60%" margin="margin: 0 20%">
                    <Image style={{width: '100%'}} of="contain" src={as3_2} />
                    <Text margin="margin-top: 10px;" style={{fontFamily: "Cormorant garamond", textAlign: 'center', fontSize: 18}}>Figure 2) Self-Attention Mechanism</Text>
                </Col>
                <p className="marginP">
                    그러다 혜성처럼 등장한 모델이 있다. 바로 Attention이다. 이 Attention은 크게 self-attention과 multi-head-attention으로 이루어지는데, 단순하다. Self-attention은 자기 자신 페어에 대해서 각각의 연결성, 즉 어떤 부분에 더 연관성이 있는지 집중(Attention)을 해보는 것이고, multi-head-attention은 자기 자신과 비교할 대상이 있는 객체에 대해 Attention을 해보는 것이다. 이를 통해서 우리는 좀 더 강화된 Context를 학습할 수 있고, 특히 context가 중요한 nlp에서는 이제 universal한 model로써 자리잡았다.
                </p>
                <p className="marginP">
                    Attention이 흥미로운 이유는 다음과 같다. 사실 간단한 아이디어인데, 이가 굉장히 universal한 task로 기대되고 있다. 결국엔 비젼이던, 자연어처리던 문맥이 바탕이 되고, 이 문맥으로 하여금 우리는 여러 확률적 approach를 통해 유추할 뿐이다. 즉 우리는 ‘문맥을 통한 사고구축’을 ‘이해’라고 할 수 있을 것이다. 
                </p>
                <p className="marginP">
                    기계던 인간이던 ‘이해’한다는 것은 매우 추상적이며, 이를 명료화시켜서 어느 기준부터 이해고, 어느 기준에 도달하지 못하면 이해가 아니라고 정의 내릴 수 없다. 다만, 우리는 특정 개념들로부터 ‘연결’을 할 수 있으며, 그 유의미한 연결을 통해서 특정한 ‘사고’를 이끌어 낼 수 있으면 그걸 이해라고 하는게 더 합리적일 것이다.
                </p>
                <Col width="60%" margin="margin: 0 20%">
                    <Image style={{width: '100%'}} of="contain" src={as3_3} />
                    <Text margin="margin-top: 10px;" style={{fontFamily: "Cormorant garamond", textAlign: 'center', fontSize: 18}}>Figure 3 ) Vision Transformer, ICLR 2021</Text>
                </Col>
                <p className="marginP">
                    이런면에서 최근의 자연어처리 분야의 발전은 매우 눈부시다. Transformer는 최근에 자연어처리에 모든 모델을 대체함에 지나지 않고, Visual Recognition 분야에서 CNN을 밀어내고 있다.
                </p>
                <p className="marginP">
                    이는 결국 ‘이해’를 한다는 것은 ‘문맥’을 안다는 것이고, 이것이 근본적으로 우리가 인간을 이해하거나 기계를 설계해야할 때 가져야할 기본적인 사고 아닐까? Transformer가 딥러닝의 universal한 task로 기대되어지는 것은 이와 별반 다르지 않을 것이다.
                </p>
                <p className="marginP"><strong>Reference</strong></p>
                <ul className="lister">
                    <li>
                        Attention is All You Need - https://arxiv.org/abs/1706.03762
                    </li>
                    <li>
                        An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale - https://arxiv.org/abs/2010.11929
                    </li>
                    <li>
                        Transformer: A Novel Neural Network Architecture for Language Understanding - https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html
                    </li>
                </ul>
            </Text>
        </Col>
    },
    {
        title: "",
        description: <Col height="100%" widht="100%" align="center" justify="center">
            <Text size="30" weight="700">감사합니다.</Text>
            <Text size="20" weight="400" style={{marginTop: 50}}>2022 Spring, Ethics of Artificial Intelligence, Web Note</Text>
        </Col>
    }
]

export default contents;